---
File: faux_lingo/__init__.py
---




---
File: faux_lingo/analysis/entropy.py
---
# faux_lingo/analysis/entropy.py
"""Information-theoretic analysis of generated sequences."""

from dataclasses import dataclass
from typing import TypeAlias

import torch
from typing_extensions import Self

from ..core.generator import GeneratedSequences
from ..core.transitions import TransitionMatrix

# Type aliases for dimensions
BatchDim: TypeAlias = int
SeqLen: TypeAlias = int
NumTopics: TypeAlias = int


@dataclass
class EntropyMetrics:
    """Container for sequence entropy measurements.

    Attributes:
        color_entropy: Empirical entropy of color transitions
        topic_entropy: Entropy of topic mixtures used in generation
        token_entropy: Empirical entropy of generated token sequences
    """

    color_entropy: float
    topic_entropy: float
    token_entropy: float

    @classmethod
    def zero(cls) -> Self:
        """Create EntropyMetrics initialized to zero."""
        return cls(
            color_entropy=0.0,
            topic_entropy=0.0,
            token_entropy=0.0,
        )


class EntropyAnalyzer:
    """Analyzer for information-theoretic properties of sequences.

    Core functionality:
    1. Computing empirical entropy of generated sequences
    2. Analyzing topic mixture entropy
    3. Tracking color transition patterns
    """

    def __init__(self, transition_model: TransitionMatrix):
        """Initialize analyzer with transition model."""
        self.transition_model = transition_model
        self.device = transition_model.device

    def analyze_sequences(
        self,
        sequences: GeneratedSequences,
    ) -> EntropyMetrics:
        """Compute entropy metrics for sequences.

        Args:
            sequences: Generated token sequences and properties

        Returns:
            EntropyMetrics containing various entropy measures
        """
        metrics = EntropyMetrics(
            color_entropy=self._compute_color_entropy(sequences.tokens),
            topic_entropy=self._compute_topic_entropy(sequences.topic_mixtures),
            token_entropy=self._compute_token_entropy(sequences.tokens),
        )

        return metrics

    def _compute_color_entropy(self, tokens: torch.Tensor) -> float:
        """Compute empirical entropy of color transitions.

        Args:
            tokens: Generated token sequences [batch, seq_len]

        Returns:
            Estimated color transition entropy
        """
        # Convert tokens to colors
        colors = torch.tensor(
            [
                [self.transition_model.color_space.get_color(idx.item()) for idx in seq]
                for seq in tokens
            ],
            device=self.device,
            dtype=torch.long,
        )

        # Count color transitions
        n_colors = self.transition_model.color_space.n_colors
        counts = torch.zeros((n_colors, n_colors), device=self.device)

        for b in range(len(tokens)):
            for t in range(len(tokens[0]) - 1):
                curr_color = colors[b, t]
                next_color = colors[b, t + 1]
                counts[curr_color, next_color] += 1

        # Convert to probabilities with row-wise normalization
        # Add small epsilon to avoid division by zero
        row_sums = counts.sum(dim=1, keepdim=True) + 1e-10
        P = counts / row_sums

        # Compute entropy per row and average
        H = -torch.sum(P * torch.log2(P + 1e-10), dim=1).mean()

        return H.item()

    def _compute_topic_entropy(self, mixtures: torch.Tensor) -> float:
        """Compute entropy of topic mixtures.

        Args:
            mixtures: Topic mixture weights [batch, n_topics]

        Returns:
            Entropy of average topic distribution
        """
        # Average topic distribution across batch
        P = mixtures.mean(0)
        H = -torch.sum(P * torch.log2(P + 1e-10))

        return H.item()

    def _compute_token_entropy(self, tokens: torch.Tensor) -> float:
        """Compute empirical entropy of token sequences.

        Args:
            tokens: Generated token sequences [batch, seq_len]

        Returns:
            Estimated token entropy
        """
        # Count token frequencies
        counts = torch.zeros(self.transition_model.vocab_size, device=self.device)

        for seq in tokens.long():  # Ensure long dtype for indexing
            unique, seq_counts = torch.unique(seq, return_counts=True)
            counts[unique] += seq_counts

        # Convert to probabilities and compute entropy
        P = counts / counts.sum()
        H = -torch.sum(P * torch.log2(P + 1e-10))

        return H.item()



---
File: faux_lingo/core/colors.py
---
# faux_lingo/core/colors.py
"""Color class management and transition rules for token sequences."""

from dataclasses import dataclass
from pathlib import Path
from typing import TypeAlias

import torch

# Type aliases for dimensions
NumColors: TypeAlias = int
VocabSize: TypeAlias = int


@dataclass
class ColorMapping:
    """Maps between token indices and color classes.

    Attributes:
        boundaries: Tensor of token index boundaries for each color
        fractions: Normalized fraction of vocabulary for each color
    """

    boundaries: torch.Tensor  # [num_colors + 1]
    fractions: torch.Tensor  # [num_colors]


class ColorSpace:
    """
    Manages color classes and their transition rules.

    Core properties:
    1. Each token belongs to exactly one color class
    2. Color classes partition the vocabulary space
    3. Transitions between colors follow specified rules
    """

    def __init__(
        self,
        color_fractions: list[float] | torch.Tensor,
        vocab_size: int,
        transition_weights: torch.Tensor | None = None,
        device: str | None = None,
    ):
        """
        Initialize color space with fractions and transition rules.

        Args:
            color_fractions: Relative sizes of color classes
            vocab_size: Total vocabulary size
            transition_weights: Optional matrix of color transition weights
            device: Optional compute device, defaults to CPU

        Notes:
            - Color fractions will be normalized to sum to 1
            - If transition_weights not provided, defaults to all-ones matrix
        """
        self.device = device if device else "cpu"

        # Convert and normalize color fractions
        if isinstance(color_fractions, list):
            color_fractions = torch.tensor(color_fractions, dtype=torch.float32)
        self.n_colors = len(color_fractions)

        # Compute normalized fractions and token boundaries
        self.mapping = self._compute_mapping(color_fractions, vocab_size)
        self.vocab_size = vocab_size

        # Setup transition weights
        if transition_weights is not None:
            self._validate_transitions(transition_weights)
            self.transition_weights = transition_weights.to(self.device)
        else:
            self.transition_weights = torch.ones(
                (self.n_colors, self.n_colors), device=self.device
            )

    def _compute_mapping(
        self, fractions: torch.Tensor, vocab_size: int
    ) -> ColorMapping:
        """
        Compute normalized fractions and token boundaries.

        Args:
            fractions: Raw color fractions
            vocab_size: Total vocabulary size

        Returns:
            ColorMapping with normalized fractions and boundaries
        """
        # Normalize fractions
        fractions = fractions.to(self.device)
        normalized = fractions / fractions.sum()

        # Compute token counts and boundaries
        counts = (normalized * vocab_size).long()

        # Adjust last count to ensure total = vocab_size
        total = counts.sum()
        if total < vocab_size:
            counts[-1] += vocab_size - total

        # Compute boundaries
        boundaries = torch.zeros(
            self.n_colors + 1, dtype=torch.long, device=self.device
        )
        torch.cumsum(counts, dim=0, out=boundaries[1:])

        return ColorMapping(boundaries=boundaries, fractions=normalized)

    def _validate_transitions(self, weights: torch.Tensor) -> None:
        """
        Validate transition weight matrix.

        Args:
            weights: Color transition weight matrix

        Raises:
            ValueError: If weights have invalid shape or values
        """
        if weights.shape != (self.n_colors, self.n_colors):
            raise ValueError(
                f"Transition weights shape {weights.shape} "
                f"doesn't match n_colors {self.n_colors}"
            )
        if not torch.all(weights >= 0):
            raise ValueError("Transition weights must be non-negative")

    def get_color(self, token_idx: int) -> int:
        """
        Get color index for a token index.

        Args:
            token_idx: Index in vocabulary

        Returns:
            Index of the color that token_idx belongs to

        Raises:
            ValueError: If token_idx is invalid
        """
        if not 0 <= token_idx < self.vocab_size:
            raise ValueError(f"Invalid token_idx {token_idx}")
        # Find which boundary region contains the token
        for i in range(self.n_colors):
            if token_idx < self.mapping.boundaries[i + 1]:
                return i
        # This should never happen due to the boundary check above
        raise RuntimeError("Failed to find color for token")

    def get_color_range(self, color_idx: int) -> tuple[int, int]:
        """
        Get token index range for a color.

        Args:
            color_idx: Index of the color

        Returns:
            Tuple of (start_idx, end_idx) for color's token range

        Raises:
            ValueError: If color_idx is invalid
        """
        if not 0 <= color_idx < self.n_colors:
            raise ValueError(f"Invalid color_idx {color_idx}")
        start = int(self.mapping.boundaries[color_idx].item())
        end = int(self.mapping.boundaries[color_idx + 1].item())
        return (start, end)

    def get_transition_mask(self) -> torch.Tensor:
        """
        Get vocabulary-sized mask from color transition weights.

        Returns:
            Boolean mask of shape [vocab_size, vocab_size]
        """
        mask = torch.zeros((self.vocab_size, self.vocab_size), device=self.device)

        for i in range(self.n_colors):
            i_start, i_end = self.get_color_range(i)
            for j in range(self.n_colors):
                j_start, j_end = self.get_color_range(j)
                if self.transition_weights[i, j] > 0:
                    mask[i_start:i_end, j_start:j_end] = self.transition_weights[i, j]

        return mask

    def save(self, path: Path) -> None:
        """Save color space parameters."""
        data = {
            "fractions": self.mapping.fractions.cpu(),
            "boundaries": self.mapping.boundaries.cpu(),
            "transition_weights": self.transition_weights.cpu(),
            "vocab_size": self.vocab_size,
        }
        torch.save(data, path)

    @classmethod
    def load(cls, path: Path, device: str | None = None) -> "ColorSpace":
        """Load color space from saved parameters."""
        data = torch.load(path)
        color_space = cls(
            color_fractions=data["fractions"],
            vocab_size=data["vocab_size"],
            transition_weights=data["transition_weights"],
            device=device,
        )
        return color_space



---
File: faux_lingo/core/generator.py
---
# faux_lingo/core/generator.py
"""Sequence generator with constrained topic and color structure."""

from dataclasses import dataclass
from typing import TypeAlias

import torch
from typing_extensions import Self

from .transitions import TransitionMatrix

# Type aliases for dimensions
BatchDim: TypeAlias = int
SeqLen: TypeAlias = int


@dataclass
class GeneratedSequences:
    """Container for generated sequences and their properties.

    Attributes:
        tokens: Generated token sequences [batch_size, seq_len]
        topic_mixtures: Topic mixtures used for generation [batch_size, n_topics]
        log_probs: Log probabilities of generated sequences [batch_size]
    """

    tokens: torch.Tensor
    topic_mixtures: torch.Tensor
    log_probs: torch.Tensor


class SequenceGenerator:
    """
    Generates sequences using topic and color-constrained transitions.

    Core functionality:
    1. Sampling sequences from transition matrices
    2. Computing sequence probabilities
    3. Generating with specific topic mixtures or color constraints
    """

    def __init__(
        self,
        transition_model: TransitionMatrix,
        device: str | None = None,
    ):
        """
        Initialize sequence generator.

        Args:
            transition_model: Model for generating transition matrices
            device: Optional compute device, defaults to CPU
        """
        self.device = device if device else "cpu"
        self.transition_model = transition_model
        self.vocab_size = transition_model.vocab_size

    def generate(
        self,
        batch_size: int,
        seq_length: int,
        temperature: float = 1.0,
        topic_mixtures: torch.Tensor | None = None,
        start_tokens: torch.Tensor | None = None,
        min_prob: float = 1e-6,
    ) -> GeneratedSequences:
        """
        Generate batch of sequences.

        Args:
            batch_size: Number of sequences to generate
            seq_length: Length of each sequence
            temperature: Controls randomness in sampling
            topic_mixtures: Optional pre-specified topic mixtures [batch_size, n_topics]
            start_tokens: Optional initial tokens [batch_size]
            min_prob: Minimum probability for valid transitions

        Returns:
            GeneratedSequences containing tokens and properties

        Notes:
            If topic_mixtures not provided, samples from uniform distribution
            If start_tokens not provided, samples initial tokens uniformly
        """
        # Get or generate topic mixtures
        if topic_mixtures is None:
            n_topics = self.transition_model.topic_space.n_topics
            topic_mixtures = torch.ones(batch_size, n_topics, device=self.device)
            topic_mixtures = topic_mixtures / n_topics

        # Validate topic mixture shape
        if topic_mixtures.shape[0] != batch_size:
            raise ValueError(
                f"Topic mixture batch size {topic_mixtures.shape[0]} "
                f"!= requested batch size {batch_size}"
            )

        # Generate transition matrix
        transitions = self.transition_model.generate(
            topic_mixtures,
            temperature=temperature,
            min_prob=min_prob,
        )

        # Initialize sequences
        sequences = torch.zeros(
            (batch_size, seq_length), dtype=torch.long, device=self.device
        )

        # Initialize log probabilities
        log_probs = torch.zeros(batch_size, device=self.device)

        # Sample or use provided start tokens
        if start_tokens is not None:
            if start_tokens.shape != (batch_size,):
                raise ValueError(
                    f"Start tokens shape {start_tokens.shape} "
                    f"!= (batch_size={batch_size},)"
                )
            sequences[:, 0] = start_tokens
        else:
            sequences[:, 0] = torch.randint(
                0, self.vocab_size, (batch_size,), device=self.device
            )

        # Generate rest of sequences
        for t in range(1, seq_length):
            # Get transition probabilities for current tokens
            current_probs = transitions[
                torch.arange(batch_size, device=self.device),
                sequences[:, t - 1],
            ]

            # Sample next tokens
            next_tokens = torch.multinomial(current_probs, 1).squeeze(-1)
            sequences[:, t] = next_tokens

            # Update log probabilities
            log_probs += torch.log(
                torch.gather(
                    current_probs,
                    1,
                    next_tokens.unsqueeze(1),
                )
            ).squeeze(-1)

        return GeneratedSequences(
            tokens=sequences,
            topic_mixtures=topic_mixtures,
            log_probs=log_probs,
        )

    def generate_with_color(
        self,
        batch_size: int,
        seq_length: int,
        start_color: int,
        temperature: float = 1.0,
        topic_mixtures: torch.Tensor | None = None,
    ) -> GeneratedSequences:
        """
        Generate sequences starting with tokens of a specific color.

        Args:
            batch_size: Number of sequences to generate
            seq_length: Length of each sequence
            start_color: Color index to start sequences with
            temperature: Controls randomness in sampling
            topic_mixtures: Optional pre-specified topic mixtures

        Returns:
            GeneratedSequences with tokens starting from specified color
        """
        # Get token range for start color
        start_idx, end_idx = self.transition_model.color_space.get_color_range(
            start_color
        )

        # Sample start tokens from color range
        start_tokens = torch.randint(
            start_idx, end_idx, (batch_size,), device=self.device
        )

        return self.generate(
            batch_size=batch_size,
            seq_length=seq_length,
            temperature=temperature,
            topic_mixtures=topic_mixtures,
            start_tokens=start_tokens,
        )

    @classmethod
    def create_uniform(
        cls,
        vocab_size: int,
        n_topics: int,
        color_fractions: list[float],
        device: str | None = None,
    ) -> Self:
        """
        Create generator with uniform topic and color distributions.

        Args:
            vocab_size: Size of token vocabulary
            n_topics: Number of topics
            color_fractions: Relative sizes of color classes
            device: Optional compute device

        Returns:
            SequenceGenerator with uniform parameters
        """
        transition_model = TransitionMatrix.create_uniform(
            vocab_size=vocab_size,
            n_topics=n_topics,
            color_fractions=color_fractions,
            device=device,
        )
        return cls(transition_model, device=device)



---
File: faux_lingo/core/serialization.py
---
# faux_lingo/core/serialization.py
"""Serialization utilities for vocabulary and generation metadata."""

from dataclasses import dataclass
from pathlib import Path
from typing import Any, TypeAlias

import torch
from loguru import logger
from omegaconf import DictConfig, OmegaConf

from .colors import ColorSpace
from .topics import TopicVectorSpace
from .transitions import TransitionMatrix
from .vocab_mapping import VocabHierarchy

# Type aliases
ConfigDict: TypeAlias = dict[str, Any]


@dataclass
class GenerationMetadata:
    """Metadata for tracking generation state and configuration.

    Attributes:
        config: Generation configuration
        vocab_hierarchy: Current vocabulary state
        transition_model: Current transition model state
        sequences_generated: Number of sequences generated
        last_batch_id: ID of last generated batch
    """

    config: DictConfig
    vocab_hierarchy: VocabHierarchy
    transition_model: TransitionMatrix
    sequences_generated: int = 0
    last_batch_id: int = 0

    def save(self, path: Path) -> None:
        """Save generation metadata to disk.

        Args:
            path: Directory to save metadata files
        """
        path.mkdir(parents=True, exist_ok=True)

        # Save configuration
        config_path = path / "config.yaml"
        OmegaConf.save(self.config, config_path)
        logger.info(f"Saved configuration to {config_path}")

        # Save vocabulary hierarchy
        vocab_path = path / "vocab"
        vocab_path.mkdir(exist_ok=True)
        for i, level in enumerate(self.vocab_hierarchy):
            level_path = vocab_path / f"level_{i}.pt"
            torch.save(level.sequences, level_path)
        logger.info(f"Saved vocabulary hierarchy to {vocab_path}")

        # Save transition model components
        model_path = path / "model"
        model_path.mkdir(exist_ok=True)

        topic_path = model_path / "topic_vectors.pt"
        self.transition_model.topic_space.save(topic_path)

        color_path = model_path / "color_space.pt"
        self.transition_model.color_space.save(color_path)
        logger.info(f"Saved transition model to {model_path}")

        # Save generation state
        state_path = path / "state.pt"
        torch.save(
            {
                "sequences_generated": self.sequences_generated,
                "last_batch_id": self.last_batch_id,
            },
            state_path,
        )
        logger.info(f"Saved generation state to {state_path}")

    @classmethod
    def load(cls, path: Path, device: str | None = None) -> "GenerationMetadata":
        """Load generation metadata from disk.

        Args:
            path: Directory containing metadata files
            device: Optional device for loading model components

        Returns:
            Loaded GenerationMetadata instance
        """
        if not path.is_dir():
            raise ValueError(f"Metadata directory {path} does not exist")

        # Load configuration
        config_path = path / "config.yaml"
        config = OmegaConf.load(config_path)
        logger.info(f"Loaded configuration from {config_path}")

        # Load vocabulary hierarchy
        vocab_path = path / "vocab"
        if not vocab_path.is_dir():
            raise ValueError(f"Vocabulary directory {vocab_path} does not exist")

        level_paths = sorted(vocab_path.glob("level_*.pt"))
        sequences = []
        for level_path in level_paths:
            sequences.append(torch.load(level_path))

        # Get chunk sizes from config
        chunk_sizes = config.vocab.chunk_sizes
        vocab_hierarchy = VocabHierarchy.from_sequences(
            sequences, chunk_sizes, device=device
        )
        logger.info(f"Loaded vocabulary hierarchy from {vocab_path}")

        # Load transition model components
        model_path = path / "model"
        if not model_path.is_dir():
            raise ValueError(f"Model directory {model_path} does not exist")

        topic_space = TopicVectorSpace.load(
            model_path / "topic_vectors.pt", device=device
        )
        color_space = ColorSpace.load(model_path / "color_space.pt", device=device)
        transition_model = TransitionMatrix(topic_space, color_space, device=device)
        logger.info(f"Loaded transition model from {model_path}")

        # Load generation state
        state_path = path / "state.pt"
        if state_path.exists():
            state = torch.load(state_path)
            sequences_generated = state["sequences_generated"]
            last_batch_id = state["last_batch_id"]
            logger.info(f"Loaded generation state from {state_path}")
        else:
            sequences_generated = 0
            last_batch_id = 0
            logger.warning(f"No generation state found at {state_path}")

        return cls(
            config=config,
            vocab_hierarchy=vocab_hierarchy,
            transition_model=transition_model,
            sequences_generated=sequences_generated,
            last_batch_id=last_batch_id,
        )



---
File: faux_lingo/core/topics.py
---
# faux_lingo/core/topics.py
"""Core functionality for topic-based sequence generation."""

from pathlib import Path
from typing import TypeAlias

import torch

# Type aliases for tensor dimensions
BatchDim: TypeAlias = int
VocabSize: TypeAlias = int
NumTopics: TypeAlias = int


class TopicVectorSpace:
    """
    Manages a set of orthonormal topic vectors that define token distributions.

    Core mathematical properties:
    1. Each topic vector is unit length
    2. All topic vectors are orthogonal to each other
    3. Topic vectors form a basis for generating token distributions
    """

    def __init__(
        self,
        n_topics: int,
        vocab_size: int,
        vectors: torch.Tensor | None = None,
        device: str | None = None,
    ):
        """
        Initialize topic vector space.

        Args:
            n_topics: Number of topics (must be <= vocab_size)
            vocab_size: Size of token vocabulary
            vectors: Optional pre-defined topic vectors
            device: Optional compute device for tensors, defaults to CPU
        """
        if n_topics > vocab_size:
            raise ValueError(
                f"n_topics ({n_topics}) must be <= vocab_size ({vocab_size})"
            )

        self.n_topics = n_topics
        self.vocab_size = vocab_size
        self.device = device if device else "cpu"

        if vectors is not None:
            self._validate_vectors(vectors)
            self.vectors = vectors.to(self.device)
        else:
            self.vectors = self._init_random_vectors()

    def _validate_vectors(self, vectors: torch.Tensor) -> None:
        """
        Validate topic vector properties.

        Args:
            vectors: Topic vectors to validate

        Raises:
            ValueError: If vectors don't meet required properties
        """
        if vectors.shape != (self.n_topics, self.vocab_size):
            raise ValueError(
                f"Vector shape {vectors.shape} doesn't match "
                f"({self.n_topics}, {self.vocab_size})"
            )

        # Check unit length
        norms = torch.linalg.norm(vectors, dim=1)
        if not torch.allclose(norms, torch.ones_like(norms)):
            raise ValueError("Topic vectors must have unit length")

        # Check orthogonality
        gram = vectors @ vectors.T
        should_be_identity = torch.eye(self.n_topics, device=vectors.device)
        if not torch.allclose(gram, should_be_identity, atol=1e-6):
            raise ValueError("Topic vectors must be orthogonal")

    def _init_random_vectors(self) -> torch.Tensor:
        """
        Initialize random orthonormal topic vectors.

        Returns:
            Tensor of orthonormal vectors
        """
        vectors = torch.randn(self.n_topics, self.vocab_size, device=self.device)
        # Use QR decomposition to get orthonormal basis
        Q, _ = torch.linalg.qr(vectors.T)
        Q_T: torch.Tensor = Q.T  # Explicit typing
        return Q_T

    def get_distribution(self, mixture: torch.Tensor) -> torch.Tensor:
        """
        Get token distribution for a topic mixture.

        Args:
            mixture: Topic mixture weights [batch_size, n_topics]

        Returns:
            Token probabilities [batch_size, vocab_size]

        Notes:
            Probabilities may need further processing (e.g., ReLU, normalization)
            to get final transition probabilities
        """
        # Move mixture to correct device
        mixture = mixture.to(self.device)

        # Validate mixture
        if mixture.shape[-1] != self.n_topics:
            raise ValueError(
                f"Mixture shape {mixture.shape} doesn't match n_topics {self.n_topics}"
            )

        # Project mixture onto topic vectors
        return mixture @ self.vectors

    def save(self, path: Path) -> None:
        """Save topic vectors."""
        # Save to CPU tensors for better compatibility
        torch.save(self.vectors.cpu(), path)

    @classmethod
    def load(cls, path: Path, device: str | None = None) -> "TopicVectorSpace":
        """Load topic vectors and construct space."""
        vectors = torch.load(path)
        n_topics, vocab_size = vectors.shape
        return cls(n_topics, vocab_size, vectors=vectors, device=device)



---
File: faux_lingo/core/transitions.py
---
# faux_lingo/core/transitions.py
"""Transition probability matrices combining topic and color constraints."""

from typing import TypeAlias

import torch
from typing_extensions import Self

from .colors import ColorSpace
from .topics import TopicVectorSpace

# Type aliases for dimensions
BatchDim: TypeAlias = int
VocabSize: TypeAlias = int


class TransitionMatrix:
    """
    Manages transition probability matrices that respect both topic and color constraints.

    Core properties:
    1. Matrices are proper probability distributions (row-wise sum to 1)
    2. Color transitions follow specified weights
    3. Global token distributions reflect topic mixtures
    """

    def __init__(
        self,
        topic_space: TopicVectorSpace,
        color_space: ColorSpace,
        device: str | None = None,
    ):
        """
        Initialize transition matrix generator.

        Args:
            topic_space: Space of topic vectors
            color_space: Color class definitions and rules
            device: Optional compute device, defaults to CPU

        Raises:
            ValueError: If spaces have incompatible dimensions
        """
        if topic_space.vocab_size != color_space.vocab_size:
            raise ValueError(
                f"Vocab size mismatch: topics ({topic_space.vocab_size}) "
                f"!= colors ({color_space.vocab_size})"
            )

        self.device = device if device else "cpu"
        self.topic_space = topic_space
        self.color_space = color_space
        self.vocab_size = topic_space.vocab_size

    def generate(
        self,
        topic_mixture: torch.Tensor,
        temperature: float = 1.0,
        min_prob: float = 1e-6,
    ) -> torch.Tensor:
        """
        Generate transition probability matrix for given topic mixture.

        Args:
            topic_mixture: Mixture weights for topics [batch_size, n_topics]
            temperature: Controls entropy of distributions (higher = more uniform)
            min_prob: Minimum probability for valid transitions

        Returns:
            Transition probability matrix [batch_size, vocab_size, vocab_size]

        Notes:
            1. Output[b,i,j] = P(token_j | token_i) for sequence b
            2. Each row sums to 1 (is a valid probability distribution)
            3. Respects both topic and color constraints
        """
        # Get base distributions from topics
        base_probs = self.topic_space.get_distribution(topic_mixture)

        # Convert to transition matrix
        # Each row i is the topic distribution masked by valid transitions from token i
        # Expand base probabilities to transition matrix shape
        transitions = base_probs.unsqueeze(1).expand(-1, self.vocab_size, -1)

        # Apply color mask to enforce transition constraints
        color_mask = self.color_space.get_transition_mask()
        transitions = transitions * color_mask

        # Set minimum probability for valid transitions
        transitions = torch.where(
            color_mask > 0,
            torch.maximum(transitions, torch.tensor(min_prob, device=self.device)),
            transitions,
        )

        # Apply temperature scaling to logits before normalization
        if temperature != 1.0:
            transitions = transitions / temperature

        # Normalize each row to get proper probability distributions
        # Small epsilon to avoid division by zero
        row_sums = transitions.sum(dim=-1, keepdim=True) + 1e-10
        transitions = transitions / row_sums

        return transitions

    @classmethod
    def create_uniform(
        cls,
        vocab_size: int,
        n_topics: int,
        color_fractions: list[float],
        device: str | None = None,
    ) -> Self:
        """
        Create transition matrix with uniform topic vectors and color transitions.

        Args:
            vocab_size: Size of token vocabulary
            n_topics: Number of topics to use
            color_fractions: Relative sizes of color classes
            device: Optional compute device

        Returns:
            TransitionMatrix instance with uniform parameters
        """
        topic_space = TopicVectorSpace(
            n_topics=n_topics,
            vocab_size=vocab_size,
            device=device,
        )
        color_space = ColorSpace(
            color_fractions=color_fractions,
            vocab_size=vocab_size,
            device=device,
        )
        return cls(topic_space, color_space, device=device)

    def save(self, path: str) -> None:
        """Save transition parameters."""
        raise NotImplementedError("Saving not yet implemented")

    @classmethod
    def load(cls, path: str, device: str | None = None) -> Self:
        """Load transition parameters."""
        raise NotImplementedError("Loading not yet implemented")



---
File: faux_lingo/core/vocab_builder.py
---
# faux_lingo/core/vocab_builder.py
"""Builder for constructing hierarchical vocabularies."""

import random
from dataclasses import dataclass
from typing import TypeAlias

from loguru import logger

from .vocab_mapping import VocabHierarchy, VocabLevel

# Type aliases
TokenIdx: TypeAlias = int
TokenSeq: TypeAlias = tuple[int, ...]


@dataclass
class BuilderConfig:
    """Configuration for vocabulary hierarchy construction.

    Attributes:
        token_vocab_size: Size of base token vocabulary
        sequence_lengths: List of sequence lengths for each level
        vocab_sizes: List of vocabulary sizes for each level
        seed: Optional random seed for reproducibility
    """

    token_vocab_size: int
    sequence_lengths: list[int]  # Each level's sequence length
    vocab_sizes: list[int]  # Each level's vocabulary size
    seed: int | None = None

    def __post_init__(self) -> None:
        """Validate configuration."""
        if self.token_vocab_size < 1:
            raise ValueError("token_vocab_size must be positive")
        if len(self.sequence_lengths) != len(self.vocab_sizes):
            raise ValueError(
                "Must specify sequence length and vocabulary size for each level"
            )
        if any(length < 1 for length in self.sequence_lengths):
            raise ValueError("All sequence lengths must be positive")
        if any(v < 1 for v in self.vocab_sizes):
            raise ValueError("All vocabulary sizes must be positive")

        # Compute and validate potential combinations at each level
        tokens = self.token_vocab_size
        for level, (length, size) in enumerate(
            zip(self.sequence_lengths, self.vocab_sizes)
        ):
            max_combinations = tokens**length
            if size > max_combinations:
                raise ValueError(
                    f"Level {level}: vocab_size ({size}) exceeds maximum "
                    f"possible combinations ({max_combinations})"
                )
            tokens = size  # Next level builds from this vocabulary


class VocabBuilder:
    """Builds hierarchical vocabularies with constrained structure.

    Core functionality:
    1. Random sampling of valid token sequences
    2. Building vocabularies level by level
    3. Tracking used sequences to avoid duplicates
    """

    def __init__(self, config: BuilderConfig):
        """Initialize builder with configuration.

        Args:
            config: Parameters for vocabulary construction
        """
        self.config = config
        self._rng = random.Random(config.seed)

        # Initialize sequence tracking
        self._used_sequences: list[set[TokenSeq]] = [
            set() for _ in range(len(config.vocab_sizes))
        ]

        logger.info("Initialized VocabBuilder with config: {}", config)

    def build(self) -> VocabHierarchy:
        """Build complete vocabulary hierarchy.

        Returns:
            VocabHierarchy with all levels constructed
        """
        levels: list[VocabLevel] = []
        current_vocab_size = self.config.token_vocab_size

        # Build each level
        for level, (seq_len, vocab_size) in enumerate(
            zip(self.config.sequence_lengths, self.config.vocab_sizes)
        ):
            logger.debug("Building level {} vocabulary...", level)

            # Generate valid sequences for this level
            sequences: dict[TokenIdx, TokenSeq] = {}
            while len(sequences) < vocab_size:
                seq = tuple(
                    self._rng.randrange(current_vocab_size) for _ in range(seq_len)
                )
                if seq not in self._used_sequences[level]:
                    token_idx = len(sequences)
                    sequences[token_idx] = seq
                    self._used_sequences[level].add(seq)

            # Create vocabulary level
            vocab_level = VocabLevel(
                vocab_size=vocab_size,
                chunk_size=seq_len,
                sequences=sequences,
            )
            levels.append(vocab_level)

            # Update for next level
            current_vocab_size = vocab_size

        return VocabHierarchy(levels)

    @classmethod
    def create_default_config(cls) -> BuilderConfig:
        """Create configuration with reasonable defaults.

        Returns:
            BuilderConfig for simple three-level hierarchy
        """
        return BuilderConfig(
            token_vocab_size=10,  # Base tokens (0-9)
            sequence_lengths=[2, 3, 2],  # Length at each level
            vocab_sizes=[20, 15, 10],  # Vocabulary sizes
        )


def create_word_hierarchy(
    token_vocab_size: int = 10,
    n_chars: int = 20,
    n_words: int = 100,
    chars_per_word: int = 3,
    seed: int | None = None,
) -> VocabHierarchy:
    """Convenience function to create character-word vocabulary.

    Args:
        token_vocab_size: Size of base token vocabulary
        n_chars: Number of unique characters
        n_words: Number of unique words
        chars_per_word: Number of characters per word
        seed: Optional random seed

    Returns:
        Two-level hierarchy mapping words to character sequences
    """
    config = BuilderConfig(
        token_vocab_size=token_vocab_size,
        sequence_lengths=[2, chars_per_word],  # tokens->chars, chars->words
        vocab_sizes=[n_chars, n_words],
        seed=seed,
    )
    return VocabBuilder(config).build()



---
File: faux_lingo/core/vocab_extensions.py
---
# faux_lingo/core/vocab_extensions.py
"""Extensions to vocabulary system for multiple mappings and augmentations."""

from dataclasses import dataclass
from typing import Sequence, TypeAlias

import torch

from .vocab_mapping import TokenIdx, TokenSeq, VocabHierarchy

# Type aliases
Probability: TypeAlias = float
AugmentedSeq: TypeAlias = tuple[TokenSeq, Probability]


@dataclass
class MultiMappingLevel:
    """Vocabulary level supporting multiple mappings.

    Attributes:
        vocab_size: Number of tokens at this level
        chunk_size: Number of tokens from parent level per token
        sequences: Mapping of token to list of possible sequences with probabilities
    """

    vocab_size: int
    chunk_size: int
    sequences: dict[TokenIdx, list[AugmentedSeq]]

    def __post_init__(self) -> None:
        """Validate level properties."""
        if self.vocab_size < 1:
            raise ValueError("vocab_size must be positive")
        if self.chunk_size < 1:
            raise ValueError("chunk_size must be positive")
        if len(self.sequences) != self.vocab_size:
            raise ValueError(
                f"Number of sequences ({len(self.sequences)}) "
                f"!= vocab_size ({self.vocab_size})"
            )

        # Validate sequence probabilities
        for token, seqs in self.sequences.items():
            if not seqs:
                raise ValueError(f"No sequences defined for token {token}")
            probs = [prob for _, prob in seqs]
            if not torch.allclose(torch.tensor(sum(probs)), torch.tensor(1.0)):
                raise ValueError(
                    f"Sequence probabilities for token {token} do not sum to 1"
                )


class MultiMappingHierarchy:
    """Hierarchical vocabulary with multiple possible mappings.

    Core functionality:
    1. Support for multiple sequences mapping to same token
    2. Probabilistic sequence selection during decoding
    3. Integration with existing vocabulary system
    """

    def __init__(
        self,
        levels: Sequence[MultiMappingLevel],
        device: str | None = None,
    ):
        """Initialize hierarchy with multiple mapping levels.

        Args:
            levels: Sequence of vocabulary levels from lowest to highest
            device: Optional compute device, defaults to CPU
        """
        self.device = device if device else "cpu"
        self.levels = list(levels)

    def decode_sequence(
        self,
        tokens: torch.Tensor,
        start_level: int,
        target_level: int,
        seed: int | None = None,
    ) -> torch.Tensor:
        """Decode token sequence with probabilistic mapping selection.

        Args:
            tokens: Input token sequence [batch_size, seq_len]
            start_level: Index of starting vocabulary level
            target_level: Index of target vocabulary level
            seed: Optional random seed for reproducible decoding

        Returns:
            Decoded token sequences at target level [batch_size, new_seq_len]
        """
        if seed is not None:
            torch.manual_seed(seed)

        if not (0 <= start_level < len(self.levels)):
            raise ValueError(f"Invalid start_level: {start_level}")
        if not (0 <= target_level < len(self.levels)):
            raise ValueError(f"Invalid target_level: {target_level}")
        if target_level > start_level:
            raise ValueError("Can only decode to same or lower levels")

        # Return input tokens if no decoding needed
        if target_level == start_level:
            return tokens

        # Start with input tokens
        current = tokens

        # Decode through intermediate levels
        for level in range(start_level - 1, target_level - 1, -1):
            # Get all possible sequences for each token
            level_seqs = self.levels[level].sequences
            max_seq_len = max(
                len(seq) for seqs in level_seqs.values() for seq, _ in seqs
            )

            # Create output tensor with padding
            result = torch.full(
                (
                    current.shape[0],
                    current.shape[1] * max_seq_len,
                ),
                -1,
                dtype=torch.long,
                device=self.device,
            )

            # Process each token in batch
            for b in range(current.shape[0]):
                pos = 0
                for t in range(current.shape[1]):
                    # Ensure integer token index
                    token_idx = int(current[b, t].item())
                    if token_idx == -1:  # Skip padding
                        continue

                    # Get possible sequences and probabilities
                    seqs = level_seqs[token_idx]
                    probs = torch.tensor([p for _, p in seqs], device=self.device)

                    # Sample sequence based on probabilities and convert to int
                    seq_idx = int(torch.multinomial(probs, 1).item())
                    seq, _ = seqs[seq_idx]

                    # Add sequence to result
                    result[b, pos : pos + len(seq)] = torch.tensor(
                        seq, device=self.device
                    )
                    pos += len(seq)

            current = result

        return current


@dataclass
class AugmentationConfig:
    """Configuration for sequence augmentation.

    Attributes:
        deletion_prob: Probability of character deletion
        insertion_prob: Probability of random character insertion
        substitution_prob: Probability of character substitution
        transposition_prob: Probability of adjacent character transposition
        seed: Optional random seed for reproducibility
    """

    deletion_prob: float = 0.05
    insertion_prob: float = 0.05
    substitution_prob: float = 0.05
    transposition_prob: float = 0.05
    seed: int | None = None

    def __post_init__(self) -> None:
        """Validate configuration."""
        probs = [
            self.deletion_prob,
            self.insertion_prob,
            self.substitution_prob,
            self.transposition_prob,
        ]
        if any(p < 0 or p > 1 for p in probs):
            raise ValueError("All probabilities must be between 0 and 1")
        if sum(probs) > 1:
            raise ValueError("Sum of probabilities must not exceed 1")


class SequenceAugmenter:
    """Applies random perturbations to token sequences.

    Core functionality:
    1. Character-level augmentations (deletion, insertion, etc.)
    2. Controlled randomization based on probabilities
    3. Vocabulary-aware modifications
    """

    def __init__(
        self,
        vocab_size: int,
        config: AugmentationConfig,
        device: str | None = None,
    ):
        """Initialize augmenter with vocabulary and configuration.

        Args:
            vocab_size: Size of token vocabulary
            config: Augmentation parameters
            device: Optional compute device, defaults to CPU
        """
        self.vocab_size = vocab_size
        self.config = config
        self.device = device if device else "cpu"

        if config.seed is not None:
            torch.manual_seed(config.seed)

    def augment_sequence(self, sequence: TokenSeq) -> TokenSeq:
        """Apply random augmentations to token sequence.

        Args:
            sequence: Input token sequence

        Returns:
            Augmented token sequence
        """
        seq = list(sequence)

        # Apply augmentations in random order
        ops = [
            (self._delete, self.config.deletion_prob),
            (self._insert, self.config.insertion_prob),
            (self._substitute, self.config.substitution_prob),
            (self._transpose, self.config.transposition_prob),
        ]

        for op, prob in ops:
            if torch.rand(1).item() < prob:
                seq = op(seq)

        return tuple(seq)

    def _delete(self, seq: list[int]) -> list[int]:
        """Randomly delete a token."""
        if len(seq) <= 1:
            return seq
        idx = int(torch.randint(len(seq), (1,)).item())
        return seq[:idx] + seq[idx + 1 :]

    def _insert(self, seq: list[int]) -> list[int]:
        """Insert random token."""
        idx = int(torch.randint(len(seq) + 1, (1,)).item())
        token = int(torch.randint(self.vocab_size, (1,)).item())
        result = seq.copy()
        result.insert(idx, token)
        return result

    def _substitute(self, seq: list[int]) -> list[int]:
        """Replace token with a different random token."""
        if not seq:
            return seq
        idx = int(torch.randint(len(seq), (1,)).item())
        current = seq[idx]
        # Generate new token until it's different from current
        while True:
            token = int(torch.randint(self.vocab_size, (1,)).item())
            if token != current:
                break
        result = seq.copy()
        result[idx] = token
        return result

    def _transpose(self, seq: list[int]) -> list[int]:
        """Swap adjacent tokens."""
        if len(seq) <= 1:
            return seq
        idx = int(torch.randint(len(seq) - 1, (1,)).item())
        result = seq.copy()
        result[idx], result[idx + 1] = result[idx + 1], result[idx]
        return result


def convert_to_multi_mapping(
    hierarchy: VocabHierarchy,
    augmenter: SequenceAugmenter | None = None,
    n_variants: int = 3,
) -> MultiMappingHierarchy:
    """Convert standard hierarchy to multi-mapping hierarchy.

    Args:
        hierarchy: Standard vocabulary hierarchy
        augmenter: Optional sequence augmenter for variants
        n_variants: Number of variants to generate per sequence

    Returns:
        MultiMappingHierarchy with original and variant sequences
    """
    levels = []

    for level in hierarchy:
        multi_sequences: dict[TokenIdx, list[AugmentedSeq]] = {}

        for token, base_seq in level.sequences.items():
            variants: list[AugmentedSeq] = [
                (base_seq, 0.6)
            ]  # Original sequence gets higher weight

            if augmenter:
                # Generate variants with augmentation
                n_aug = min(n_variants - 1, 5)  # Cap number of variants
                prob_per_variant = (1.0 - 0.6) / n_aug

                for _ in range(n_aug):
                    variant = augmenter.augment_sequence(base_seq)
                    variants.append((variant, prob_per_variant))

            multi_sequences[token] = variants

        multi_level = MultiMappingLevel(
            vocab_size=level.vocab_size,
            chunk_size=level.chunk_size,
            sequences=multi_sequences,
        )
        levels.append(multi_level)

    return MultiMappingHierarchy(levels)



---
File: faux_lingo/core/vocab_mapping.py
---
# faux_lingo/core/vocab_mapping.py
"""Hierarchical vocabulary mapping and decoding."""

from dataclasses import dataclass
from typing import Iterator, Sequence, TypeAlias

import torch
from typing_extensions import Self

# Type aliases for clarity
TokenIdx: TypeAlias = int
TokenSeq: TypeAlias = tuple[int, ...]
Shape: TypeAlias = tuple[int, ...]


@dataclass
class VocabLevel:
    """
    A single level in the vocabulary hierarchy.
        Attributes:
        vocab_size: Number of tokens at this level
        chunk_size: Number of tokens from parent level per token
        sequences: Mapping of each token to its constituent sequence
    """

    vocab_size: int
    chunk_size: int
    sequences: dict[TokenIdx, TokenSeq]

    def __post_init__(self) -> None:
        """Validate vocabulary level properties."""
        if self.vocab_size < 1:
            raise ValueError("vocab_size must be positive")
        if self.chunk_size < 1:
            raise ValueError("chunk_size must be positive")
        if len(self.sequences) != self.vocab_size:
            raise ValueError(
                f"Number of sequences ({len(self.sequences)}) "
                f"!= vocab_size ({self.vocab_size})"
            )
        # Validate all sequences are proper tuples of integers
        for token, seq in self.sequences.items():
            if not isinstance(seq, tuple):
                raise ValueError(f"Sequence for token {token} must be a tuple")
            if not all(isinstance(t, int) for t in seq):
                raise ValueError(
                    f"All elements in sequence for token {token} must be integers"
                )

    @property
    def max_sequence_length(self) -> int:
        """Maximum length of any sequence in this level."""
        return max(len(seq) for seq in self.sequences.values())


class VocabHierarchy:
    """
    Manages hierarchical relationships between vocabulary levels.

    Note: VocabLevels represent mappings BETWEEN levels, not the levels themselves.
    With n VocabLevels, we actually have n+1 vocabulary levels total.
    Level indexing goes from most abstract (0) to most concrete (n):
    Level 0 -> Level 1 (Mapping A)
    Level 1 -> Level 2 (Mapping B)
    """

    def __init__(
        self,
        levels: Sequence[VocabLevel],
        device: str | None = None,
    ) -> None:
        """Initialize vocabulary hierarchy.

        Args:
            levels: Sequence of vocabulary mappings from highest to lowest abstraction
            device: Optional compute device, defaults to CPU
        """
        self.device = device if device else "cpu"
        self.levels = list(levels)
        self.num_levels = len(self.levels) + 1
        self.decode_tables = self._build_decode_tables()

    def decode_sequence(
        self,
        tokens: torch.Tensor,
        start_level: int | None = None,
        target_level: int | None = None,
    ) -> torch.Tensor:
        """Decode token sequence from one level to another.

        Args:
            tokens: Input token sequence [batch_size, seq_len]
            start_level: Optional starting level (defaults to 0)
            target_level: Optional target level (defaults to max level)
        Returns:
            Decoded token sequences at target level [batch_size, new_seq_len]
        """
        # Default to decoding from top level to bottom level
        if start_level is None:
            start_level = 0
        if target_level is None:
            target_level = self.num_levels - 1

        if not (0 <= start_level < self.num_levels):
            raise ValueError(f"Invalid start_level: {start_level}")
        if not (0 <= target_level < self.num_levels):
            raise ValueError(f"Invalid target_level: {target_level}")
        if target_level < start_level:
            raise ValueError("Can only decode to same or higher levels")

        # Return input tokens if no decoding needed
        if target_level == start_level:
            return tokens

        # Start with input tokens
        current = tokens

        # Decode through intermediate levels
        for level in range(start_level, target_level):
            table = self.decode_tables[level]
            decoded = table[current]

            # Remove padding and flatten sequence
            mask = decoded != -1
            lengths = mask.sum(dim=-1)
            max_length = int(lengths.sum(dim=-1).max().item())

            # Create output tensor with proper shape and type
            result = torch.full(
                size=(decoded.shape[0], max_length),
                fill_value=-1,
                dtype=torch.long,
                device=self.device,
            )

            # Fill in decoded sequences
            pos = 0
            for i in range(decoded.shape[1]):
                seq_lengths = lengths[:, i]
                for b in range(decoded.shape[0]):
                    length = int(seq_lengths[b].item())
                    if length > 0:
                        result[b, pos : pos + length] = decoded[b, i, :length]
                pos += int(seq_lengths.max().item())

            current = result

        return current[current != -1].view(tokens.shape[0], -1)

    def _build_decode_tables(self) -> list[torch.Tensor]:
        """Build lookup tables for decoding between levels.

        Returns:
            List of tensors mapping level i tokens to level i+1 sequences
            Each tensor has shape [parent_vocab_size, max_child_sequence_length]
            with padded sequences for consistent shape
        """
        tables = []
        for level in self.levels:
            max_length = max(len(seq) for seq in level.sequences.values())

            table = torch.full(
                size=(level.vocab_size, max_length),
                fill_value=-1,
                dtype=torch.long,
                device=self.device,
            )

            for token, sequence in level.sequences.items():
                table[token, : len(sequence)] = torch.tensor(
                    sequence, dtype=torch.long, device=self.device
                )

            tables.append(table)

        return tables

    @classmethod
    def from_sequences(
        cls,
        sequences: list[dict[TokenIdx, TokenSeq]],
        chunk_sizes: list[int],
        device: str | None = None,
    ) -> Self:
        """Create hierarchy from sequence mappings.
        Args:
            sequences: Mappings for each level
            chunk_sizes: Number of tokens per chunk at each level
            device: Optional compute device
        Returns:
            Initialized VocabHierarchy
        """
        if len(sequences) != len(chunk_sizes):
            raise ValueError("Must provide chunk size for each level")

        levels = []
        for seq_map, chunk_size in zip(sequences, chunk_sizes):
            level = VocabLevel(
                vocab_size=len(seq_map),
                chunk_size=chunk_size,
                sequences=seq_map,
            )
            levels.append(level)

        return cls(levels, device=device)

    def __getitem__(self, level: int) -> VocabLevel:
        """Get vocabulary level by index."""
        return self.levels[level]

    def __len__(self) -> int:
        """Get number of vocabulary levels."""
        return len(self.levels)

    def __iter__(self) -> Iterator[VocabLevel]:
        """Iterate over vocabulary levels."""
        return iter(self.levels)



---
File: faux_lingo/data/dataset.py
---
# faux_lingo/data/dataset.py
"""Dataset management for sequence generation."""

from dataclasses import dataclass
from typing import Iterator, TypeAlias, TypedDict

import torch

from ..core.generator import GeneratedSequences, SequenceGenerator

# Type aliases for dimensions
BatchDim: TypeAlias = int
SeqLen: TypeAlias = int


class BatchStats(TypedDict):
    mean_log_prob: float
    topic_weights: list[float]
    color_counts: list[int]


@dataclass
class DatasetConfig:
    """Configuration for dataset generation.

    Attributes:
        batch_size: Number of sequences per batch
        seq_length: Length of each sequence
        n_batches: Total number of batches to generate
        temperature: Controls randomness in generation
        seed: Random seed for reproducibility
    """

    batch_size: int
    seq_length: int
    n_batches: int
    temperature: float = 1.0
    seed: int | None = None


class SequenceDataset:
    """Manages generation and iteration of sequence batches.

    Core functionality:
    1. Batch generation with consistent configuration
    2. Tracking of sequence properties and metadata
    3. Iterator interface for training/validation
    """

    def __init__(
        self,
        generator: SequenceGenerator,
        config: DatasetConfig,
    ):
        """Initialize dataset with generator and configuration.

        Args:
            generator: Sequence generator instance
            config: Dataset generation parameters
        """
        self.generator = generator
        self.config = config
        self.device = generator.device

        # Set random seed if provided
        if config.seed is not None:
            torch.manual_seed(config.seed)

        # Initialize dataset properties
        self.total_sequences = config.batch_size * config.n_batches
        self._current_batch = 0
        self._cached_batch: GeneratedSequences | None = None

    def __len__(self) -> int:
        """Get total number of batches."""
        return self.config.n_batches

    def __iter__(self) -> Iterator[GeneratedSequences]:
        """Create iterator over sequence batches."""
        return self

    def __next__(self) -> GeneratedSequences:
        """Get next batch of sequences."""
        if self._current_batch >= self.config.n_batches:
            self._current_batch = 0
            raise StopIteration

        self._current_batch += 1
        return self.generate_batch()

    def generate_batch(
        self,
        topic_mixtures: torch.Tensor | None = None,
        start_color: int | None = None,
    ) -> GeneratedSequences:
        """Generate a single batch of sequences.

        Args:
            topic_mixtures: Optional pre-specified topic mixtures
            start_color: Optional color index to start sequences with

        Returns:
            GeneratedSequences containing tokens and properties
        """
        if start_color is not None:
            sequences = self.generator.generate_with_color(
                batch_size=self.config.batch_size,
                seq_length=self.config.seq_length,
                start_color=start_color,
                temperature=self.config.temperature,
                topic_mixtures=topic_mixtures,
            )
        else:
            sequences = self.generator.generate(
                batch_size=self.config.batch_size,
                seq_length=self.config.seq_length,
                temperature=self.config.temperature,
                topic_mixtures=topic_mixtures,
            )

        self._cached_batch = sequences
        return sequences

    def get_color_sequences(self, tokens: torch.Tensor) -> torch.Tensor:
        """Convert token sequences to color sequences.

        Args:
            tokens: Token sequences [batch_size, seq_length]

        Returns:
            Color sequences [batch_size, seq_length]
        """
        return torch.tensor(
            [
                [
                    self.generator.transition_model.color_space.get_color(idx.item())
                    for idx in seq
                ]
                for seq in tokens
            ],
            device=self.device,
            dtype=torch.long,
        )

    def get_batch_stats(self, batch: GeneratedSequences) -> BatchStats:
        """Compute statistics for a batch of sequences.

        Args:
            batch: Batch of generated sequences

        Returns:
            Dictionary of batch statistics
        """
        color_seqs = self.get_color_sequences(batch.tokens)

        stats: BatchStats = {
            "mean_log_prob": batch.log_probs.mean().item(),
            "topic_weights": batch.topic_mixtures.mean(0).tolist(),
            "color_counts": torch.bincount(
                color_seqs.view(-1),
                minlength=self.generator.transition_model.color_space.n_colors,
            ).tolist(),
        }

        return stats

    @property
    def vocab_size(self) -> int:
        """Get vocabulary size of generator."""
        return self.generator.vocab_size

    @property
    def n_topics(self) -> int:
        """Get number of topics in generator."""
        return self.generator.transition_model.topic_space.n_topics

    @property
    def n_colors(self) -> int:
        """Get number of color classes in generator."""
        return self.generator.transition_model.color_space.n_colors


