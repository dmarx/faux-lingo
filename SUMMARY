---
File: .github/workflows/ci.yml
---
name: CI

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:

permissions:
  contents: write
  pull-requests: write

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
      with:
        fetch-depth: 0
        ref: ${{ github.head_ref }}
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.12"
        
    - name: Cache pip
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-pip-
          
    - name: Install base package
      run: |
        python -m pip install --upgrade pip
        echo "::group::Install Timing - Base Package"
        time pip install -e .
        echo "::endgroup::"
        
    - name: Install dev dependencies
      run: |
        echo "::group::Install Timing - Dev Dependencies"
        time pip install -e ".[dev]"
        echo "::endgroup::"
        
    - name: Run formatters and linters
      continue-on-error: true
      run: |
        echo "Running black..."
        black faux_lingo
        echo "Running isort..."
        isort faux_lingo
        echo "Running ruff..."
        ruff check faux_lingo --fix .
        
    - name: Commit formatting changes
      if: github.event_name == 'pull_request'
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        if ! git diff --quiet; then
          git add .
          git commit -m "Apply automatic formatting and type fixes"
          git push origin HEAD:${{ github.head_ref }}
        fi
        
    - name: Run tests
      run: |
        pytest --cov  
  
    - name: Run type checking (informational)
      continue-on-error: true
      run: |
        echo "=== mypy Type Check Report ===" >> $GITHUB_STEP_SUMMARY
        mypy faux_lingo --pretty | tee -a $GITHUB_STEP_SUMMARY || true
    
    - name: Run linting (informational)
      continue-on-error: true
      run: |
        echo "=== Ruff Lint Report ===" >> $GITHUB_STEP_SUMMARY
        ruff check faux_lingo | tee -a $GITHUB_STEP_SUMMARY || true



---
File: .github/workflows/llamero.yml
---
name: Llamero Summarization

on:
  #push:
  workflow_dispatch:

jobs:
  generate-summaries:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write

    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'

    - name: Install llamero
      run: pip install llamero

    - name: Generate summaries
      run: llamero summarize all



---
File: README.md
---
![FauxLingo: Simulate Syntax and Semantics](static/sequence_generation.svg)

# FauxLingo

### 1. Introduction

FauxLingo is a powerful toolkit for generating structured token sequences with controlled properties. It combines topic-based generation with color-based constraints to produce sequences that exhibit specific statistical and structural patterns.

#### Key Features and Capabilities
- Hierarchical vocabulary system with multiple abstraction levels
- Topic-based sequence generation with controlled mixing
- Color-based constraints for structural patterns
- Information-theoretic analysis tools
- Extensible augmentation system for sequence variants
- Robust serialization for long-running generations

#### Common Use Cases
- Generating synthetic training data with controlled properties
- Testing sequence processing systems
- Studying information-theoretic properties of structured sequences
- Prototyping language models with controlled vocabularies

### 2. Getting Started

#### Basic Concepts

##### Tokens and Vocabularies
FauxLingo organizes tokens into hierarchical vocabularies. A token is the basic unit of sequence generation, and tokens can be organized into higher-level structures. For example:
```python
# Create a simple word-level hierarchy
hierarchy = create_word_hierarchy(
    token_vocab_size=10,  # Base tokens (0-9)
    n_chars=20,          # Number of character-level tokens
    n_words=100,         # Number of word-level tokens
    chars_per_word=3     # Characters per word
)
```

##### Colors and Topics
- Colors are classes that partition the token vocabulary and control transition patterns
- Topics are basis vectors that define token distributions
- Together they create structured yet flexible sequence generation

##### Sequence Generation
Basic sequence generation combines topic mixtures with color constraints:
```python
generator = SequenceGenerator.create_uniform(
    vocab_size=100,
    n_topics=3,
    color_fractions=[0.3, 0.3, 0.4]
)

sequences = generator.generate(
    batch_size=32,
    seq_length=20,
    temperature=1.0
)
```

#### Quick Start Tutorial
1. Install FauxLingo:
   ```bash
   pip install fauxlingo
   ```

2. Create a simple generator:
   ```python
   from faux_lingo import SequenceGenerator
   
   generator = SequenceGenerator.create_uniform(
       vocab_size=10,
       n_topics=2,
       color_fractions=[0.5, 0.5]
   )
   ```

3. Generate sequences:
   ```python
   sequences = generator.generate(
       batch_size=4,
       seq_length=10
   )
   ```

4. Analyze results:
   ```python
   from faux_lingo.analysis import EntropyAnalyzer
   
   analyzer = EntropyAnalyzer(generator.transition_model)
   metrics = analyzer.analyze_sequences(sequences)
   print(f"Token entropy: {metrics.token_entropy:.2f}")
   ```

#### Common Use Patterns
- Generate -> Analyze -> Adjust -> Repeat
- Build vocabulary -> Generate sequences -> Export
- Load existing -> Augment -> Generate variants

### 3. Core Components

#### 3.1 Vocabulary System

##### Understanding Vocabulary Hierarchy
The vocabulary system organizes tokens into levels:
```
Level 2 (Words):    [Word_0] [Word_1] ...
                      ↓         ↓
Level 1 (Chars):  [C_0,C_1] [C_2,C_3] ...
                    ↓   ↓     ↓   ↓
Level 0 (Tokens): [0,1,2] [3,4,5] ...
```

##### Creating Custom Vocabularies
```python
from faux_lingo.core.vocab_builder import BuilderConfig, VocabBuilder

config = BuilderConfig(
    token_vocab_size=10,
    sequence_lengths=[2, 3],  # Length at each level
    vocab_sizes=[20, 30]      # Size of each level
)

builder = VocabBuilder(config)
hierarchy = builder.build()
```

##### Multiple Mappings and Variants
Support for multiple sequences mapping to the same token:
```python
from faux_lingo.core.vocab_extensions import (
    AugmentationConfig,
    SequenceAugmenter,
    convert_to_multi_mapping
)

augmenter = SequenceAugmenter(
    vocab_size=10,
    config=AugmentationConfig(
        deletion_prob=0.1,
        insertion_prob=0.1
    )
)

multi_hierarchy = convert_to_multi_mapping(
    hierarchy,
    augmenter=augmenter,
    n_variants=3
)
```

##### Best Practices for Vocabulary Design
- Keep hierarchy levels focused and logical
- Balance vocabulary sizes between levels
- Consider sequence length impact on combinations
- Use meaningful partitioning for colors

#### 3.2 Generation System

##### Basic Sequence Generation
```python
# Simple generation with default settings
sequences = generator.generate(
    batch_size=32,
    seq_length=20
)

# Control randomness with temperature
sequences = generator.generate(
    batch_size=32,
    seq_length=20,
    temperature=0.8  # Lower = more deterministic
)
```

##### Topic-based Generation
```python
# Generate with specific topic mixture
topic_mixture = torch.tensor([[0.7, 0.3]])  # Favor first topic
sequences = generator.generate(
    batch_size=1,
    seq_length=20,
    topic_mixtures=topic_mixture
)
```

##### Color Constraints
```python
# Generate sequences starting with specific color
sequences = generator.generate_with_color(
    batch_size=32,
    seq_length=20,
    start_color=1  # Start with second color class
)
```

#### 3.3 Analysis Tools

##### Entropy Metrics
```python
analyzer = EntropyAnalyzer(generator.transition_model)
metrics = analyzer.analyze_sequences(sequences)

print(f"Color entropy: {metrics.color_entropy:.2f}")
print(f"Topic entropy: {metrics.topic_entropy:.2f}")
print(f"Token entropy: {metrics.token_entropy:.2f}")
```

##### Sequence Analysis
```python
# Get color sequences
dataset = SequenceDataset(generator, config)
color_seqs = dataset.get_color_sequences(sequences.tokens)

# Get batch statistics
stats = dataset.get_batch_stats(sequences)
print(f"Mean log probability: {stats['mean_log_prob']:.2f}")
print(f"Color distribution: {stats['color_counts']}")
```

### 4. Advanced Usage

#### Configuration System
```python
from faux_lingo.core.serialization import GenerationMetadata
from omegaconf import OmegaConf

# Create configuration
config = OmegaConf.create({
    "generation": {
        "batch_size": 32,
        "seq_length": 20,
        "temperature": 0.8
    },
    "vocab": {
        "token_size": 100,
        "n_topics": 3,
        "color_fractions": [0.3, 0.3, 0.4]
    }
})

# Create metadata container
metadata = GenerationMetadata(
    config=config,
    vocab_hierarchy=hierarchy,
    transition_model=generator.transition_model
)
```

#### Serialization and State Management
```python
# Save state
metadata.save(Path("generation_state"))

# Load state
metadata = GenerationMetadata.load(
    Path("generation_state"),
    device="cuda"
)
```

#### Augmentation System
```python
config = AugmentationConfig(
    deletion_prob=0.1,
    insertion_prob=0.1,
    substitution_prob=0.1,
    transposition_prob=0.1
)

augmenter = SequenceAugmenter(
    vocab_size=100,
    config=config
)

# Augment sequence
sequence = (0, 1, 2, 3)
variant = augmenter.augment_sequence(sequence)
```

### 5. Examples

#### Basic Examples

##### Simple Vocabulary Creation
```python
# Create word-level hierarchy
hierarchy = create_word_hierarchy(
    token_vocab_size=10,
    n_chars=20,
    n_words=100,
    chars_per_word=3
)
```

##### Basic Generation
```python
# Create generator
generator = SequenceGenerator.create_uniform(
    vocab_size=100,
    n_topics=3,
    color_fractions=[0.3, 0.3, 0.4]
)

# Generate sequences
sequences = generator.generate(
    batch_size=32,
    seq_length=20
)
```

##### Analysis Examples
```python
# Analyze entropy
analyzer = EntropyAnalyzer(generator.transition_model)
metrics = analyzer.analyze_sequences(sequences)

# Get batch statistics
dataset = SequenceDataset(generator, config)
stats = dataset.get_batch_stats(sequences)
```

#### Advanced Examples

##### Complex Hierarchies
```python
# Multi-level hierarchy with custom configuration
config = BuilderConfig(
    token_vocab_size=10,
    sequence_lengths=[2, 3, 2],  # Three levels
    vocab_sizes=[20, 30, 15]     # Sizes at each level
)

builder = VocabBuilder(config)
hierarchy = builder.build()
```

##### Custom Constraints
```python
# Create custom color transition weights
weights = torch.tensor([
    [1.0, 0.5, 0.0],  # Color 0 transitions
    [0.5, 1.0, 0.5],  # Color 1 transitions
    [0.0, 0.5, 1.0]   # Color 2 transitions
])

color_space = ColorSpace(
    color_fractions=[0.3, 0.3, 0.4],
    vocab_size=100,
    transition_weights=weights
)
```

##### Integration Examples
```python
# Integrate with training loop
for epoch in range(n_epochs):
    for batch in SequenceDataset(generator, config):
        # Process batch
        tokens = batch.tokens
        topic_mixtures = batch.topic_mixtures
        log_probs = batch.log_probs
        
        # Training step
        loss = model(tokens, topic_mixtures)
        loss.backward()
```

### 6. Troubleshooting

#### Common Error Messages

1. "Vocab size mismatch":
   - Check that vocabulary sizes match between components
   - Verify color space and topic space use same vocab size

2. "Invalid topic mixture":
   - Ensure topic mixtures sum to 1.0
   - Check batch dimension matches requested batch size

3. "Transition weights shape mismatch":
   - Verify transition weight matrix matches number of colors
   - Check that weights are non-negative

4. "Sequence probabilities do not sum to 1":
   - In multi-mapping, check variant probabilities sum to 1.0
   - Verify normalization in transition matrices

For additional support and resources:
- Check the GitHub repository
- Review test cases for examples
- File issues for bugs or feature requests

---

# Appendix H: Mathematics

## H.1 Probability Models

### Topic Space
The topic space is constructed using orthonormal vectors that form a basis for generating token distributions. For a vocabulary of size V and T topics:

1. **Topic Vectors**: Each topic i is represented by a unit vector vi ∈ ℝᵛ where:
   - ‖vi‖₂ = 1 (unit length)
   - vi · vj = 0 for i ≠ j (orthogonality)

2. **Topic Mixtures**: A topic mixture w = (w₁, ..., wₜ) satisfies:
   - wᵢ ≥ 0 for all i
   - Σwᵢ = 1

3. **Token Distribution**: For a topic mixture w, the base token distribution p is:
   p = Σ(wᵢvᵢ) for i = 1 to T

### Color-Constrained Transitions

1. **Color Classes**: The vocabulary is partitioned into C color classes where:
   - Each token belongs to exactly one color
   - Color c contains nc tokens
   - Σnc = V (total vocabulary size)

2. **Transition Matrix**: For a token distribution p, the transition matrix P is:
   P(j|i) = p(j) * M(c(i),c(j)) / Z(i)
   where:
   - c(i) is the color of token i
   - M is the color transition weight matrix
   - Z(i) is the normalization factor

3. **Normalization**: Z(i) ensures each row sums to 1:
   Z(i) = Σ(p(j) * M(c(i),c(j))) for all j

### Temperature Scaling
Temperature T modifies the transition probabilities:
P'(j|i) = P(j|i)^(1/T) / Z'(i)
where Z'(i) is the new normalization factor.

## H.2 Entropy Calculations

### Color Entropy
The empirical entropy of color transitions is:
H(C) = -Σ P(j|i) log₂ P(j|i)
averaged over all color pairs (i,j).

### Topic Entropy
For a batch of topic mixtures {wᵦ}, the entropy is:
H(T) = -Σ w̄ᵢ log₂ w̄ᵢ
where w̄ᵢ is the mean weight for topic i.

### Token Entropy
The empirical entropy of token sequences:
H(V) = -Σ f(v) log₂ f(v)
where f(v) is the observed frequency of token v.

## H.3 Sequence Generation

### Sampling Process
1. Given a topic mixture w and current token i:
   a. Compute base distribution p = Σ(wᵢvᵢ)
   b. Apply color constraints to get P(j|i)
   c. Apply temperature scaling
   d. Sample next token from resulting distribution

2. Batch Generation:
   - Independent samples for each sequence
   - Shared topic mixture within batch
   - Parallel computation of transitions

# Appendix B: Configuration Reference

## B.1 Available Settings

### Generation Configuration
```yaml
generation:
  batch_size: 32          # Number of sequences per batch
  seq_length: 20          # Length of each sequence
  temperature: 1.0        # Sampling temperature (default: 1.0)
  min_prob: 1e-6         # Minimum transition probability
  device: "cuda"         # Compute device (optional)
```

### Vocabulary Configuration
```yaml
vocab:
  token_vocab_size: 100   # Base vocabulary size
  sequence_lengths:       # Length of sequences at each level
    - 2                  # Level 1
    - 3                  # Level 2
  vocab_sizes:           # Vocabulary size at each level
    - 20                # Level 1
    - 30                # Level 2
  chunk_sizes:           # Tokens per chunk at each level
    - 2                 # Level 1
    - 3                 # Level 2
```

### Topic Configuration
```yaml
topics:
  n_topics: 3            # Number of topic vectors
  init_method: "random"  # Initialization method
  orthogonalize: true    # Force orthogonality
```

### Color Configuration
```yaml
colors:
  fractions:             # Relative size of color classes
    - 0.3               # Color 1
    - 0.3               # Color 2
    - 0.4               # Color 3
  transition_weights:    # Optional color transition matrix
    - [1.0, 0.5, 0.0]  # Color 1 transitions
    - [0.5, 1.0, 0.5]  # Color 2 transitions
    - [0.0, 0.5, 1.0]  # Color 3 transitions
```

### Augmentation Configuration
```yaml
augmentation:
  deletion_prob: 0.05    # Character deletion probability
  insertion_prob: 0.05   # Character insertion probability
  substitution_prob: 0.05 # Character substitution probability
  transposition_prob: 0.05 # Character transposition probability
  seed: null             # Random seed (optional)
```

## B.2 Default Values

### Core Defaults
```yaml
generation:
  batch_size: 32
  seq_length: 20
  temperature: 1.0
  min_prob: 1e-6
  device: "cpu"

vocab:
  token_vocab_size: 10
  sequence_lengths: [2, 3]
  vocab_sizes: [20, 15]
  chunk_sizes: [2, 3]

topics:
  n_topics: 2
  init_method: "random"
  orthogonalize: true

colors:
  fractions: [0.5, 0.5]
  transition_weights: null  # Defaults to all-ones matrix
```

### Augmentation Defaults
```yaml
augmentation:
  deletion_prob: 0.05
  insertion_prob: 0.05
  substitution_prob: 0.05
  transposition_prob: 0.05
  seed: null
```

## B.3 Environment Variables

The following environment variables can override configuration settings:

```bash
FAUXLINGO_DEVICE="cuda"          # Override compute device
FAUXLINGO_BATCH_SIZE="64"        # Override batch size
FAUXLINGO_TEMPERATURE="0.8"      # Override temperature
FAUXLINGO_SEED="42"             # Set random seed
```

## B.4 Configuration Validation

The configuration system validates:
1. Numeric ranges (e.g., probabilities between 0 and 1)
2. Compatibility between components
3. Resource requirements (e.g., memory constraints)
4. Device availability

Configuration errors include detailed messages explaining the validation failure and suggested fixes.

# Mathematics and Code Correlation

## 1. Topic Space

### Mathematical Definition
A topic space consists of T orthonormal vectors in ℝᵛ where V is the vocabulary size.

**Topic Vector Properties**:
- Unit length: ‖vi‖₂ = 1
- Orthogonality: vi · vj = 0 for i ≠ j

### Code Implementation
In `core/topics.py`:
```python
class TopicVectorSpace:
    def _validate_vectors(self, vectors: torch.Tensor) -> None:
        # Check unit length
        norms = torch.linalg.norm(vectors, dim=1)
        if not torch.allclose(norms, torch.ones_like(norms)):
            raise ValueError("Topic vectors must have unit length")

        # Check orthogonality
        gram = vectors @ vectors.T
        should_be_identity = torch.eye(self.n_topics, device=vectors.device)
        if not torch.allclose(gram, should_be_identity, atol=1e-6):
            raise ValueError("Topic vectors must be orthogonal")
```

### Distribution Generation
**Math**: p = Σ(wᵢvᵢ) for i = 1 to T

**Code**:
```python
def get_distribution(self, mixture: torch.Tensor) -> torch.Tensor:
    """Project mixture onto topic vectors."""
    return mixture @ self.vectors
```

## 2. Color-Constrained Transitions

### Mathematical Definition
For token distribution p and color transition matrix M:
P(j|i) = p(j) * M(c(i),c(j)) / Z(i)

where Z(i) = Σ(p(j) * M(c(i),c(j))) is the normalization factor.

### Code Implementation
In `core/transitions.py`:
```python
class TransitionMatrix:
    def generate(
        self,
        topic_mixture: torch.Tensor,
        temperature: float = 1.0,
        min_prob: float = 1e-6,
    ) -> torch.Tensor:
        # Get base distributions from topics
        base_probs = self.topic_space.get_distribution(topic_mixture)

        # Convert to transition matrix
        transitions = base_probs.unsqueeze(1).expand(-1, self.vocab_size, -1)

        # Apply color mask
        color_mask = self.color_space.get_transition_mask()
        transitions = transitions * color_mask

        # Set minimum probability for valid transitions
        transitions = torch.where(
            color_mask > 0,
            torch.maximum(transitions, torch.tensor(min_prob)),
            transitions
        )

        # Normalize probabilities
        row_sums = transitions.sum(dim=-1, keepdim=True) + 1e-10
        transitions = transitions / row_sums
```

### Color Space Implementation
In `core/colors.py`:
```python
class ColorSpace:
    def get_transition_mask(self) -> torch.Tensor:
        """Create vocabulary-sized mask from color transitions."""
        mask = torch.zeros(
            (self.vocab_size, self.vocab_size),
            device=self.device
        )

        for i in range(self.n_colors):
            i_start, i_end = self.get_color_range(i)
            for j in range(self.n_colors):
                j_start, j_end = self.get_color_range(j)
                if self.transition_weights[i, j] > 0:
                    mask[i_start:i_end, j_start:j_end] = (
                        self.transition_weights[i, j]
                    )
```

## 3. Entropy Calculations

### Color Entropy
**Math**: H(C) = -Σ P(j|i) log₂ P(j|i)

**Code** in `analysis/entropy.py`:
```python
def _compute_color_entropy(self, tokens: torch.Tensor) -> float:
    # Convert tokens to colors
    colors = torch.tensor(
        [[self.transition_model.color_space.get_color(idx.item())
          for idx in seq] for seq in tokens],
        device=self.device,
    )

    # Count transitions
    counts = torch.zeros(
        (self.transition_model.color_space.n_colors,
         self.transition_model.color_space.n_colors),
        device=self.device
    )

    for b in range(len(tokens)):
        for t in range(len(tokens[0]) - 1):
            curr_color = colors[b, t]
            next_color = colors[b, t + 1]
            counts[curr_color, next_color] += 1

    # Convert to probabilities
    row_sums = counts.sum(dim=1, keepdim=True) + 1e-10
    P = counts / row_sums

    # Compute entropy
    H = -torch.sum(P * torch.log2(P + 1e-10), dim=1).mean()
    return H.item()
```

### Topic Entropy
**Math**: H(T) = -Σ w̄ᵢ log₂ w̄ᵢ

**Code**:
```python
def _compute_topic_entropy(self, mixtures: torch.Tensor) -> float:
    # Average topic distribution
    P = mixtures.mean(0)
    H = -torch.sum(P * torch.log2(P + 1e-10))
    return H.item()
```

### Token Entropy
**Math**: H(V) = -Σ f(v) log₂ f(v)

**Code**:
```python
def _compute_token_entropy(self, tokens: torch.Tensor) -> float:
    # Count frequencies
    counts = torch.zeros(
        self.transition_model.vocab_size,
        device=self.device
    )

    for seq in tokens.long():
        unique, seq_counts = torch.unique(seq, return_counts=True)
        counts[unique] += seq_counts

    # Convert to probabilities and compute entropy
    P = counts / counts.sum()
    H = -torch.sum(P * torch.log2(P + 1e-10))
    return H.item()
```

## 4. Sequence Generation

### Sampling Process
**Math**: 
1. p = Σ(wᵢvᵢ)  # Base distribution
2. P(j|i) = p(j) * M(c(i),c(j)) / Z(i)  # Apply constraints
3. P'(j|i) = P(j|i)^(1/T) / Z'(i)  # Temperature scaling

### Code Implementation
In `core/generator.py`:
```python
class SequenceGenerator:
    def generate(
        self,
        batch_size: int,
        seq_length: int,
        temperature: float = 1.0,
        topic_mixtures: torch.Tensor | None = None,
        start_tokens: torch.Tensor | None = None,
        min_prob: float = 1e-6,
    ) -> GeneratedSequences:
        # Get transition matrix
        transitions = self.transition_model.generate(
            topic_mixtures,
            temperature=temperature,
            min_prob=min_prob,
        )

        sequences = torch.zeros(
            (batch_size, seq_length),
            dtype=torch.long,
            device=self.device
        )
        log_probs = torch.zeros(batch_size, device=self.device)

        # Initial tokens
        if start_tokens is None:
            sequences[:, 0] = torch.randint(
                0, self.vocab_size,
                (batch_size,),
                device=self.device
            )
        else:
            sequences[:, 0] = start_tokens

        # Generate sequence
        for t in range(1, seq_length):
            current_probs = transitions[
                torch.arange(batch_size, device=self.device),
                sequences[:, t - 1],
            ]

            # Sample next tokens
            next_tokens = torch.multinomial(current_probs, 1).squeeze(-1)
            sequences[:, t] = next_tokens

            # Update log probabilities
            log_probs += torch.log(
                torch.gather(
                    current_probs,
                    1,
                    next_tokens.unsqueeze(1),
                )
            ).squeeze(-1)
```

This implementation shows how:
1. Topic vectors define the base distribution
2. Color constraints are applied via masking
3. Temperature scaling controls randomness
4. Sequences are generated through sequential sampling

The code carefully handles:
- Numerical stability (adding small epsilon)
- Efficient batch processing
- Device placement
- Memory management
- Edge cases in probability distributions

Each mathematical operation has corresponding validation in the code to ensure theoretical properties are maintained during computation.

---

# Mathematics and Code Correlation

## 1. Topic Space and Vector Basis

### Theoretical Foundation
The topic space is constructed as a low-dimensional representation of token distributions using orthonormal vectors. This approach is grounded in:

1. **Linear Subspace Theory**:
   - Topics form a T-dimensional subspace of ℝᵛ
   - Any token distribution is a convex combination of topic vectors
   - Linear independence ensures unique representations

2. **Gram-Schmidt Process**:
   For topic vectors {vᵢ}, the orthonormalization process:
   ```
   ṽ₁ = v₁
   ṽᵢ = vᵢ - Σⱼ₌₁ⁱ⁻¹ proj_ṽⱼ(vᵢ)
   vᵢ' = ṽᵢ/‖ṽᵢ‖
   ```
   where proj_u(v) = (u·v)/(u·u)u

3. **Basis Properties**:
   - Completeness: span{vᵢ} covers allowed distributions
   - Parsimony: minimal basis for required expressivity
   - Interpretability: each vector represents distinct pattern

### Geometric Interpretation
The topic simplex is formed by:
- Vertices: pure topic distributions
- Edges: binary topic mixtures
- Interior: general topic mixtures

This structure ensures:
- Non-negative probabilities
- Proper normalization
- Continuous interpolation between topics

### Code Implementation
In `core/topics.py`:
```python
class TopicVectorSpace:
    def _validate_vectors(self, vectors: torch.Tensor) -> None:
        # Check unit length
        norms = torch.linalg.norm(vectors, dim=1)
        if not torch.allclose(norms, torch.ones_like(norms)):
            raise ValueError("Topic vectors must have unit length")

        # Check orthogonality
        gram = vectors @ vectors.T
        should_be_identity = torch.eye(self.n_topics, device=vectors.device)
        if not torch.allclose(gram, should_be_identity, atol=1e-6):
            raise ValueError("Topic vectors must be orthogonal")
```

### Distribution Generation
**Math**: p = Σ(wᵢvᵢ) for i = 1 to T

**Code**:
```python
def get_distribution(self, mixture: torch.Tensor) -> torch.Tensor:
    """Project mixture onto topic vectors."""
    return mixture @ self.vectors
```

## 2. Color-Constrained Transitions and Markov Properties

### Theoretical Framework

1. **Markov Chain Structure**:
   - State space: S = {1,...,V} (vocabulary tokens)
   - Transition kernel: P(j|i) with color constraints
   - Stationary distribution: π satisfying πP = π

2. **Block Structure**:
   The transition matrix has block form:
   ```
   P = [P₁₁ P₁₂ ... P₁ₖ]
       [P₂₁ P₂₂ ... P₂ₖ]
       [  ...........   ]
       [Pₖ₁ Pₖ₂ ... Pₖₖ]
   ```
   where Pᵢⱼ represents transitions from color i to color j

3. **Constraint Properties**:
   For colors c(i), c(j):
   - Block sparsity: P(j|i) = 0 if M(c(i),c(j)) = 0
   - Block scaling: P(j|i) ∝ M(c(i),c(j))
   - Row normalization: ΣⱼP(j|i) = 1

4. **Ergodicity Conditions**:
   - Irreducibility: M allows paths between all colors
   - Aperiodicity: diagonal M entries positive
   - Positive recurrence: finite state space

### Code Implementation
In `core/transitions.py`:
```python
class TransitionMatrix:
    def generate(
        self,
        topic_mixture: torch.Tensor,
        temperature: float = 1.0,
        min_prob: float = 1e-6,
    ) -> torch.Tensor:
        # Get base distributions from topics
        base_probs = self.topic_space.get_distribution(topic_mixture)

        # Convert to transition matrix
        transitions = base_probs.unsqueeze(1).expand(-1, self.vocab_size, -1)

        # Apply color mask
        color_mask = self.color_space.get_transition_mask()
        transitions = transitions * color_mask

        # Set minimum probability for valid transitions
        transitions = torch.where(
            color_mask > 0,
            torch.maximum(transitions, torch.tensor(min_prob)),
            transitions
        )

        # Normalize probabilities
        row_sums = transitions.sum(dim=-1, keepdim=True) + 1e-10
        transitions = transitions / row_sums
```

## 3. Information Theory and Entropy Analysis

### Theoretical Framework

1. **Shannon Entropy Fundamentals**:
   For a discrete random variable X:
   H(X) = -Σₓ P(x) log₂ P(x)

   Properties:
   - Non-negativity: H(X) ≥ 0
   - Maximality: H uniform = log₂|X|
   - Additivity: H(X,Y) = H(X) + H(Y|X)

2. **Conditional Entropy**:
   For color transitions:
   H(C₂|C₁) = -Σᵢ,ⱼ P(i,j) log₂ P(j|i)
   where:
   - P(i,j): joint probability of colors i,j
   - P(j|i): conditional probability

3. **Mutual Information**:
   Between colors and topics:
   I(C;T) = H(C) - H(C|T)
   = Σᵢ,ⱼ P(i,j) log₂(P(i,j)/(P(i)P(j)))

4. **Entropy Rate**:
   For the sequence process:
   h = lim(n→∞) H(Xₙ|Xₙ₋₁,...,X₁)/n

### Estimation Methods

1. **Empirical Entropy**:
   Ĥ = -Σᵢ (nᵢ/N) log₂(nᵢ/N)
   where:
   - nᵢ: count of event i
   - N: total observations

2. **Bias Correction**:
   Miller-Madow correction:
   Ĥ_c = Ĥ + (m-1)/(2N)
   where m is number of non-zero counts

### Code Implementation
In `analysis/entropy.py`:
```python
def _compute_color_entropy(self, tokens: torch.Tensor) -> float:
    # Convert tokens to colors
    colors = torch.tensor(
        [[self.transition_model.color_space.get_color(idx.item())
          for idx in seq] for seq in tokens],
        device=self.device,
    )

    # Count transitions
    counts = torch.zeros(
        (self.transition_model.color_space.n_colors,
         self.transition_model.color_space.n_colors),
        device=self.device
    )

    for b in range(len(tokens)):
        for t in range(len(tokens[0]) - 1):
            curr_color = colors[b, t]
            next_color = colors[b, t + 1]
            counts[curr_color, next_color] += 1

    # Convert to probabilities
    row_sums = counts.sum(dim=1, keepdim=True) + 1e-10
    P = counts / row_sums

    # Compute entropy
    H = -torch.sum(P * torch.log2(P + 1e-10), dim=1).mean()
    return H.item()
```

## 4. Sequence Generation and Sampling Theory

### Theoretical Framework

1. **Gibbs Distribution**:
   With temperature T, probability:
   P_T(x) = exp(-E(x)/T)/Z_T
   where:
   - E(x): energy function
   - Z_T: partition function
   - T: temperature parameter

2. **Temperature Effects**:
   Limiting behavior:
   - T → 0: converges to mode (deterministic)
   - T → ∞: converges to uniform
   - T = 1: original distribution

3. **Sampling Methods**:

   a) **Direct Sampling**:
      For categorical distribution P:
      - Compute cumulative sums Sᵢ = Σⱼ≤ᵢ pⱼ
      - Generate u ~ Uniform(0,1)
      - Return min{i: Sᵢ > u}

   b) **Gumbel-Max Trick**:
      For logits l:
      - Generate g ~ Gumbel(0,1)
      - Return argmax(l + g)
      
   c) **Importance Sampling**:
      When sampling from P using Q:
      w(x) = P(x)/Q(x)
      Ê[f(X)] = Σᵢ w(xᵢ)f(xᵢ)/Σᵢ w(xᵢ)

4. **Convergence Properties**:

   a) **Mixing Time**:
      τ(ε) = min{t: ‖P^t - π‖ᵤ ≤ ε}
      where:
      - P^t: t-step transition matrix
      - π: stationary distribution
      - ‖·‖ᵤ: uniform norm

   b) **Spectral Gap**:
      γ = 1 - λ₂
      where λ₂ is second largest eigenvalue

   c) **Conductance**:
      Φ = min{Φ(S): 0 < π(S) ≤ 1/2}
      where Φ(S) = P(S,S^c)/π(S)

### Implementation Considerations

1. **Numerical Stability**:
   - Log-space computations
   - Softmax with temperature
   - Epsilon for division

2. **Batch Processing**:
   - Parallel sampling
   - Memory efficiency
   - Device optimization

3. **Variance Reduction**:
   - Common random numbers
   - Antithetic variates
   - Control variates

### Code Implementation
In `core/generator.py`:
```python
class SequenceGenerator:
    def generate(
        self,
        batch_size: int,
        seq_length: int,
        temperature: float = 1.0,
        topic_mixtures: torch.Tensor | None = None,
        start_tokens: torch.Tensor | None = None,
        min_prob: float = 1e-6,
    ) -> GeneratedSequences:
        # Get transition matrix
        transitions = self.transition_model.generate(
            topic_mixtures,
            temperature=temperature,
            min_prob=min_prob,
        )

        sequences = torch.zeros(
            (batch_size, seq_length),
            dtype=torch.long,
            device=self.device
        )
        log_probs = torch.zeros(batch_size, device=self.device)

        # Generate sequence
        for t in range(1, seq_length):
            current_probs = transitions[
                torch.arange(batch_size, device=self.device),
                sequences[:, t - 1],
            ]

            # Sample next tokens
            next_tokens = torch.multinomial(current_probs, 1).squeeze(-1)
            sequences[:, t] = next_tokens

            # Update log probabilities
            log_probs += torch.log(
                torch.gather(
                    current_probs,
                    1,
                    next_tokens.unsqueeze(1),
                )
            ).squeeze(-1)
```



---
File: faux_lingo/__init__.py
---




---
File: faux_lingo/analysis/entropy.py
---
# faux_lingo/analysis/entropy.py
"""Information-theoretic analysis of generated sequences."""

from dataclasses import dataclass
from typing import TypeAlias

import torch
from typing_extensions import Self

from ..core.generator import GeneratedSequences
from ..core.transitions import TransitionMatrix

# Type aliases for dimensions
BatchDim: TypeAlias = int
SeqLen: TypeAlias = int
NumTopics: TypeAlias = int


@dataclass
class EntropyMetrics:
    """Container for sequence entropy measurements.

    Attributes:
        color_entropy: Empirical entropy of color transitions
        topic_entropy: Entropy of topic mixtures used in generation
        token_entropy: Empirical entropy of generated token sequences
    """

    color_entropy: float
    topic_entropy: float
    token_entropy: float

    @classmethod
    def zero(cls) -> Self:
        """Create EntropyMetrics initialized to zero."""
        return cls(
            color_entropy=0.0,
            topic_entropy=0.0,
            token_entropy=0.0,
        )


class EntropyAnalyzer:
    """Analyzer for information-theoretic properties of sequences.

    Core functionality:
    1. Computing empirical entropy of generated sequences
    2. Analyzing topic mixture entropy
    3. Tracking color transition patterns
    """

    def __init__(self, transition_model: TransitionMatrix):
        """Initialize analyzer with transition model."""
        self.transition_model = transition_model
        self.device = transition_model.device

    def analyze_sequences(
        self,
        sequences: GeneratedSequences,
    ) -> EntropyMetrics:
        """Compute entropy metrics for sequences.

        Args:
            sequences: Generated token sequences and properties

        Returns:
            EntropyMetrics containing various entropy measures
        """
        metrics = EntropyMetrics(
            color_entropy=self._compute_color_entropy(sequences.tokens),
            topic_entropy=self._compute_topic_entropy(sequences.topic_mixtures),
            token_entropy=self._compute_token_entropy(sequences.tokens),
        )

        return metrics

    def _compute_color_entropy(self, tokens: torch.Tensor) -> float:
        """Compute empirical entropy of color transitions.

        Args:
            tokens: Generated token sequences [batch, seq_len]

        Returns:
            Estimated color transition entropy
        """
        # Convert tokens to colors
        colors = torch.tensor(
            [
                [self.transition_model.color_space.get_color(idx.item()) for idx in seq]
                for seq in tokens
            ],
            device=self.device,
            dtype=torch.long,
        )

        # Count color transitions
        n_colors = self.transition_model.color_space.n_colors
        counts = torch.zeros((n_colors, n_colors), device=self.device)

        for b in range(len(tokens)):
            for t in range(len(tokens[0]) - 1):
                curr_color = colors[b, t]
                next_color = colors[b, t + 1]
                counts[curr_color, next_color] += 1

        # Convert to probabilities with row-wise normalization
        # Add small epsilon to avoid division by zero
        row_sums = counts.sum(dim=1, keepdim=True) + 1e-10
        P = counts / row_sums

        # Compute entropy per row and average
        H = -torch.sum(P * torch.log2(P + 1e-10), dim=1).mean()

        return H.item()

    def _compute_topic_entropy(self, mixtures: torch.Tensor) -> float:
        """Compute entropy of topic mixtures.

        Args:
            mixtures: Topic mixture weights [batch, n_topics]

        Returns:
            Entropy of average topic distribution
        """
        # Average topic distribution across batch
        P = mixtures.mean(0)
        H = -torch.sum(P * torch.log2(P + 1e-10))

        return H.item()

    def _compute_token_entropy(self, tokens: torch.Tensor) -> float:
        """Compute empirical entropy of token sequences.

        Args:
            tokens: Generated token sequences [batch, seq_len]

        Returns:
            Estimated token entropy
        """
        # Count token frequencies
        counts = torch.zeros(self.transition_model.vocab_size, device=self.device)

        for seq in tokens.long():  # Ensure long dtype for indexing
            unique, seq_counts = torch.unique(seq, return_counts=True)
            counts[unique] += seq_counts

        # Convert to probabilities and compute entropy
        P = counts / counts.sum()
        H = -torch.sum(P * torch.log2(P + 1e-10))

        return H.item()



---
File: faux_lingo/core/colors.py
---
# faux_lingo/core/colors.py
"""Color class management and transition rules for token sequences."""

from dataclasses import dataclass
from pathlib import Path
from typing import TypeAlias

import torch

# Type aliases for dimensions
NumColors: TypeAlias = int
VocabSize: TypeAlias = int


@dataclass
class ColorMapping:
    """Maps between token indices and color classes.

    Attributes:
        boundaries: Tensor of token index boundaries for each color
        fractions: Normalized fraction of vocabulary for each color
    """

    boundaries: torch.Tensor  # [num_colors + 1]
    fractions: torch.Tensor  # [num_colors]


class ColorSpace:
    """
    Manages color classes and their transition rules.

    Core properties:
    1. Each token belongs to exactly one color class
    2. Color classes partition the vocabulary space
    3. Transitions between colors follow specified rules
    """

    def __init__(
        self,
        color_fractions: list[float] | torch.Tensor,
        vocab_size: int,
        transition_weights: torch.Tensor | None = None,
        device: str | None = None,
    ):
        """
        Initialize color space with fractions and transition rules.

        Args:
            color_fractions: Relative sizes of color classes
            vocab_size: Total vocabulary size
            transition_weights: Optional matrix of color transition weights
            device: Optional compute device, defaults to CPU

        Notes:
            - Color fractions will be normalized to sum to 1
            - If transition_weights not provided, defaults to all-ones matrix
        """
        self.device = device if device else "cpu"

        # Convert and normalize color fractions
        if isinstance(color_fractions, list):
            color_fractions = torch.tensor(color_fractions, dtype=torch.float32)
        self.n_colors = len(color_fractions)

        # Compute normalized fractions and token boundaries
        self.mapping = self._compute_mapping(color_fractions, vocab_size)
        self.vocab_size = vocab_size

        # Setup transition weights
        if transition_weights is not None:
            self._validate_transitions(transition_weights)
            self.transition_weights = transition_weights.to(self.device)
        else:
            self.transition_weights = torch.ones(
                (self.n_colors, self.n_colors), device=self.device
            )

    def _compute_mapping(
        self, fractions: torch.Tensor, vocab_size: int
    ) -> ColorMapping:
        """
        Compute normalized fractions and token boundaries.

        Args:
            fractions: Raw color fractions
            vocab_size: Total vocabulary size

        Returns:
            ColorMapping with normalized fractions and boundaries
        """
        # Normalize fractions
        fractions = fractions.to(self.device)
        normalized = fractions / fractions.sum()

        # Compute token counts and boundaries
        counts = (normalized * vocab_size).long()

        # Adjust last count to ensure total = vocab_size
        total = counts.sum()
        if total < vocab_size:
            counts[-1] += vocab_size - total

        # Compute boundaries
        boundaries = torch.zeros(
            self.n_colors + 1, dtype=torch.long, device=self.device
        )
        torch.cumsum(counts, dim=0, out=boundaries[1:])

        return ColorMapping(boundaries=boundaries, fractions=normalized)

    def _validate_transitions(self, weights: torch.Tensor) -> None:
        """
        Validate transition weight matrix.

        Args:
            weights: Color transition weight matrix

        Raises:
            ValueError: If weights have invalid shape or values
        """
        if weights.shape != (self.n_colors, self.n_colors):
            raise ValueError(
                f"Transition weights shape {weights.shape} "
                f"doesn't match n_colors {self.n_colors}"
            )
        if not torch.all(weights >= 0):
            raise ValueError("Transition weights must be non-negative")

    def get_color(self, token_idx: int) -> int:
        """
        Get color index for a token index.

        Args:
            token_idx: Index in vocabulary

        Returns:
            Index of the color that token_idx belongs to

        Raises:
            ValueError: If token_idx is invalid
        """
        if not 0 <= token_idx < self.vocab_size:
            raise ValueError(f"Invalid token_idx {token_idx}")
        # Find which boundary region contains the token
        for i in range(self.n_colors):
            if token_idx < self.mapping.boundaries[i + 1]:
                return i
        # This should never happen due to the boundary check above
        raise RuntimeError("Failed to find color for token")

    def get_color_range(self, color_idx: int) -> tuple[int, int]:
        """
        Get token index range for a color.

        Args:
            color_idx: Index of the color

        Returns:
            Tuple of (start_idx, end_idx) for color's token range

        Raises:
            ValueError: If color_idx is invalid
        """
        if not 0 <= color_idx < self.n_colors:
            raise ValueError(f"Invalid color_idx {color_idx}")
        start = int(self.mapping.boundaries[color_idx].item())
        end = int(self.mapping.boundaries[color_idx + 1].item())
        return (start, end)

    def get_transition_mask(self) -> torch.Tensor:
        """
        Get vocabulary-sized mask from color transition weights.

        Returns:
            Boolean mask of shape [vocab_size, vocab_size]
        """
        mask = torch.zeros((self.vocab_size, self.vocab_size), device=self.device)

        for i in range(self.n_colors):
            i_start, i_end = self.get_color_range(i)
            for j in range(self.n_colors):
                j_start, j_end = self.get_color_range(j)
                if self.transition_weights[i, j] > 0:
                    mask[i_start:i_end, j_start:j_end] = self.transition_weights[i, j]

        return mask

    def save(self, path: Path) -> None:
        """Save color space parameters."""
        data = {
            "fractions": self.mapping.fractions.cpu(),
            "boundaries": self.mapping.boundaries.cpu(),
            "transition_weights": self.transition_weights.cpu(),
            "vocab_size": self.vocab_size,
        }
        torch.save(data, path)

    @classmethod
    def load(cls, path: Path, device: str | None = None) -> "ColorSpace":
        """Load color space from saved parameters."""
        data = torch.load(path)
        color_space = cls(
            color_fractions=data["fractions"],
            vocab_size=data["vocab_size"],
            transition_weights=data["transition_weights"],
            device=device,
        )
        return color_space



---
File: faux_lingo/core/generator.py
---
# faux_lingo/core/generator.py
"""Sequence generator with constrained topic and color structure."""

from dataclasses import dataclass
from typing import TypeAlias

import torch
from typing_extensions import Self

from .transitions import TransitionMatrix

# Type aliases for dimensions
BatchDim: TypeAlias = int
SeqLen: TypeAlias = int


@dataclass
class GeneratedSequences:
    """Container for generated sequences and their properties.

    Attributes:
        tokens: Generated token sequences [batch_size, seq_len]
        topic_mixtures: Topic mixtures used for generation [batch_size, n_topics]
        log_probs: Log probabilities of generated sequences [batch_size]
    """

    tokens: torch.Tensor
    topic_mixtures: torch.Tensor
    log_probs: torch.Tensor


class SequenceGenerator:
    """
    Generates sequences using topic and color-constrained transitions.

    Core functionality:
    1. Sampling sequences from transition matrices
    2. Computing sequence probabilities
    3. Generating with specific topic mixtures or color constraints
    """

    def __init__(
        self,
        transition_model: TransitionMatrix,
        device: str | None = None,
    ):
        """
        Initialize sequence generator.

        Args:
            transition_model: Model for generating transition matrices
            device: Optional compute device, defaults to CPU
        """
        self.device = device if device else "cpu"
        self.transition_model = transition_model
        self.vocab_size = transition_model.vocab_size

    def generate(
        self,
        batch_size: int,
        seq_length: int,
        temperature: float = 1.0,
        topic_mixtures: torch.Tensor | None = None,
        start_tokens: torch.Tensor | None = None,
        min_prob: float = 1e-6,
    ) -> GeneratedSequences:
        """
        Generate batch of sequences.

        Args:
            batch_size: Number of sequences to generate
            seq_length: Length of each sequence
            temperature: Controls randomness in sampling
            topic_mixtures: Optional pre-specified topic mixtures [batch_size, n_topics]
            start_tokens: Optional initial tokens [batch_size]
            min_prob: Minimum probability for valid transitions

        Returns:
            GeneratedSequences containing tokens and properties

        Notes:
            If topic_mixtures not provided, samples from uniform distribution
            If start_tokens not provided, samples initial tokens uniformly
        """
        # Get or generate topic mixtures
        if topic_mixtures is None:
            n_topics = self.transition_model.topic_space.n_topics
            topic_mixtures = torch.ones(batch_size, n_topics, device=self.device)
            topic_mixtures = topic_mixtures / n_topics

        # Validate topic mixture shape
        if topic_mixtures.shape[0] != batch_size:
            raise ValueError(
                f"Topic mixture batch size {topic_mixtures.shape[0]} "
                f"!= requested batch size {batch_size}"
            )

        # Generate transition matrix
        transitions = self.transition_model.generate(
            topic_mixtures,
            temperature=temperature,
            min_prob=min_prob,
        )

        # Initialize sequences
        sequences = torch.zeros(
            (batch_size, seq_length), dtype=torch.long, device=self.device
        )

        # Initialize log probabilities
        log_probs = torch.zeros(batch_size, device=self.device)

        # Sample or use provided start tokens
        if start_tokens is not None:
            if start_tokens.shape != (batch_size,):
                raise ValueError(
                    f"Start tokens shape {start_tokens.shape} "
                    f"!= (batch_size={batch_size},)"
                )
            sequences[:, 0] = start_tokens
        else:
            sequences[:, 0] = torch.randint(
                0, self.vocab_size, (batch_size,), device=self.device
            )

        # Generate rest of sequences
        for t in range(1, seq_length):
            # Get transition probabilities for current tokens
            current_probs = transitions[
                torch.arange(batch_size, device=self.device),
                sequences[:, t - 1],
            ]

            # Sample next tokens
            next_tokens = torch.multinomial(current_probs, 1).squeeze(-1)
            sequences[:, t] = next_tokens

            # Update log probabilities
            log_probs += torch.log(
                torch.gather(
                    current_probs,
                    1,
                    next_tokens.unsqueeze(1),
                )
            ).squeeze(-1)

        return GeneratedSequences(
            tokens=sequences,
            topic_mixtures=topic_mixtures,
            log_probs=log_probs,
        )

    def generate_with_color(
        self,
        batch_size: int,
        seq_length: int,
        start_color: int,
        temperature: float = 1.0,
        topic_mixtures: torch.Tensor | None = None,
    ) -> GeneratedSequences:
        """
        Generate sequences starting with tokens of a specific color.

        Args:
            batch_size: Number of sequences to generate
            seq_length: Length of each sequence
            start_color: Color index to start sequences with
            temperature: Controls randomness in sampling
            topic_mixtures: Optional pre-specified topic mixtures

        Returns:
            GeneratedSequences with tokens starting from specified color
        """
        # Get token range for start color
        start_idx, end_idx = self.transition_model.color_space.get_color_range(
            start_color
        )

        # Sample start tokens from color range
        start_tokens = torch.randint(
            start_idx, end_idx, (batch_size,), device=self.device
        )

        return self.generate(
            batch_size=batch_size,
            seq_length=seq_length,
            temperature=temperature,
            topic_mixtures=topic_mixtures,
            start_tokens=start_tokens,
        )

    @classmethod
    def create_uniform(
        cls,
        vocab_size: int,
        n_topics: int,
        color_fractions: list[float],
        device: str | None = None,
    ) -> Self:
        """
        Create generator with uniform topic and color distributions.

        Args:
            vocab_size: Size of token vocabulary
            n_topics: Number of topics
            color_fractions: Relative sizes of color classes
            device: Optional compute device

        Returns:
            SequenceGenerator with uniform parameters
        """
        transition_model = TransitionMatrix.create_uniform(
            vocab_size=vocab_size,
            n_topics=n_topics,
            color_fractions=color_fractions,
            device=device,
        )
        return cls(transition_model, device=device)



---
File: faux_lingo/core/serialization.py
---
# faux_lingo/core/serialization.py
"""Serialization utilities for vocabulary and generation metadata."""

from dataclasses import dataclass
from pathlib import Path
from typing import Any, TypeAlias

import torch
from loguru import logger
from omegaconf import DictConfig, OmegaConf

from .colors import ColorSpace
from .topics import TopicVectorSpace
from .transitions import TransitionMatrix
from .vocab_mapping import VocabHierarchy

# Type aliases
ConfigDict: TypeAlias = dict[str, Any]


@dataclass
class GenerationMetadata:
    """Metadata for tracking generation state and configuration.

    Attributes:
        config: Generation configuration
        vocab_hierarchy: Current vocabulary state
        transition_model: Current transition model state
        sequences_generated: Number of sequences generated
        last_batch_id: ID of last generated batch
    """

    config: DictConfig
    vocab_hierarchy: VocabHierarchy
    transition_model: TransitionMatrix
    sequences_generated: int = 0
    last_batch_id: int = 0

    def save(self, path: Path) -> None:
        """Save generation metadata to disk.

        Args:
            path: Directory to save metadata files
        """
        path.mkdir(parents=True, exist_ok=True)

        # Save configuration
        config_path = path / "config.yaml"
        OmegaConf.save(self.config, config_path)
        logger.info(f"Saved configuration to {config_path}")

        # Save vocabulary hierarchy
        vocab_path = path / "vocab"
        vocab_path.mkdir(exist_ok=True)
        for i, level in enumerate(self.vocab_hierarchy):
            level_path = vocab_path / f"level_{i}.pt"
            torch.save(level.sequences, level_path)
        logger.info(f"Saved vocabulary hierarchy to {vocab_path}")

        # Save transition model components
        model_path = path / "model"
        model_path.mkdir(exist_ok=True)

        topic_path = model_path / "topic_vectors.pt"
        self.transition_model.topic_space.save(topic_path)

        color_path = model_path / "color_space.pt"
        self.transition_model.color_space.save(color_path)
        logger.info(f"Saved transition model to {model_path}")

        # Save generation state
        state_path = path / "state.pt"
        torch.save(
            {
                "sequences_generated": self.sequences_generated,
                "last_batch_id": self.last_batch_id,
            },
            state_path,
        )
        logger.info(f"Saved generation state to {state_path}")

    @classmethod
    def load(cls, path: Path, device: str | None = None) -> "GenerationMetadata":
        """Load generation metadata from disk.

        Args:
            path: Directory containing metadata files
            device: Optional device for loading model components

        Returns:
            Loaded GenerationMetadata instance
        """
        if not path.is_dir():
            raise ValueError(f"Metadata directory {path} does not exist")

        # Load configuration
        config_path = path / "config.yaml"
        config = OmegaConf.load(config_path)
        logger.info(f"Loaded configuration from {config_path}")

        # Load vocabulary hierarchy
        vocab_path = path / "vocab"
        if not vocab_path.is_dir():
            raise ValueError(f"Vocabulary directory {vocab_path} does not exist")

        level_paths = sorted(vocab_path.glob("level_*.pt"))
        sequences = []
        for level_path in level_paths:
            sequences.append(torch.load(level_path))

        # Get chunk sizes from config
        chunk_sizes = config.vocab.chunk_sizes
        vocab_hierarchy = VocabHierarchy.from_sequences(
            sequences, chunk_sizes, device=device
        )
        logger.info(f"Loaded vocabulary hierarchy from {vocab_path}")

        # Load transition model components
        model_path = path / "model"
        if not model_path.is_dir():
            raise ValueError(f"Model directory {model_path} does not exist")

        topic_space = TopicVectorSpace.load(
            model_path / "topic_vectors.pt", device=device
        )
        color_space = ColorSpace.load(model_path / "color_space.pt", device=device)
        transition_model = TransitionMatrix(topic_space, color_space, device=device)
        logger.info(f"Loaded transition model from {model_path}")

        # Load generation state
        state_path = path / "state.pt"
        if state_path.exists():
            state = torch.load(state_path)
            sequences_generated = state["sequences_generated"]
            last_batch_id = state["last_batch_id"]
            logger.info(f"Loaded generation state from {state_path}")
        else:
            sequences_generated = 0
            last_batch_id = 0
            logger.warning(f"No generation state found at {state_path}")

        return cls(
            config=config,
            vocab_hierarchy=vocab_hierarchy,
            transition_model=transition_model,
            sequences_generated=sequences_generated,
            last_batch_id=last_batch_id,
        )



---
File: faux_lingo/core/topics.py
---
# faux_lingo/core/topics.py
"""Core functionality for topic-based sequence generation."""

from pathlib import Path
from typing import TypeAlias

import torch

# Type aliases for tensor dimensions
BatchDim: TypeAlias = int
VocabSize: TypeAlias = int
NumTopics: TypeAlias = int


class TopicVectorSpace:
    """
    Manages a set of orthonormal topic vectors that define token distributions.

    Core mathematical properties:
    1. Each topic vector is unit length
    2. All topic vectors are orthogonal to each other
    3. Topic vectors form a basis for generating token distributions
    """

    def __init__(
        self,
        n_topics: int,
        vocab_size: int,
        vectors: torch.Tensor | None = None,
        device: str | None = None,
    ):
        """
        Initialize topic vector space.

        Args:
            n_topics: Number of topics (must be <= vocab_size)
            vocab_size: Size of token vocabulary
            vectors: Optional pre-defined topic vectors
            device: Optional compute device for tensors, defaults to CPU
        """
        if n_topics > vocab_size:
            raise ValueError(
                f"n_topics ({n_topics}) must be <= vocab_size ({vocab_size})"
            )

        self.n_topics = n_topics
        self.vocab_size = vocab_size
        self.device = device if device else "cpu"

        if vectors is not None:
            self._validate_vectors(vectors)
            self.vectors = vectors.to(self.device)
        else:
            self.vectors = self._init_random_vectors()

    def _validate_vectors(self, vectors: torch.Tensor) -> None:
        """
        Validate topic vector properties.

        Args:
            vectors: Topic vectors to validate

        Raises:
            ValueError: If vectors don't meet required properties
        """
        if vectors.shape != (self.n_topics, self.vocab_size):
            raise ValueError(
                f"Vector shape {vectors.shape} doesn't match "
                f"({self.n_topics}, {self.vocab_size})"
            )

        # Check unit length
        norms = torch.linalg.norm(vectors, dim=1)
        if not torch.allclose(norms, torch.ones_like(norms)):
            raise ValueError("Topic vectors must have unit length")

        # Check orthogonality
        gram = vectors @ vectors.T
        should_be_identity = torch.eye(self.n_topics, device=vectors.device)
        if not torch.allclose(gram, should_be_identity, atol=1e-6):
            raise ValueError("Topic vectors must be orthogonal")

    def _init_random_vectors(self) -> torch.Tensor:
        """
        Initialize random orthonormal topic vectors.

        Returns:
            Tensor of orthonormal vectors
        """
        vectors = torch.randn(self.n_topics, self.vocab_size, device=self.device)
        # Use QR decomposition to get orthonormal basis
        Q, _ = torch.linalg.qr(vectors.T)
        Q_T: torch.Tensor = Q.T  # Explicit typing
        return Q_T

    def get_distribution(self, mixture: torch.Tensor) -> torch.Tensor:
        """
        Get token distribution for a topic mixture.

        Args:
            mixture: Topic mixture weights [batch_size, n_topics]

        Returns:
            Token probabilities [batch_size, vocab_size]

        Notes:
            Probabilities may need further processing (e.g., ReLU, normalization)
            to get final transition probabilities
        """
        # Move mixture to correct device
        mixture = mixture.to(self.device)

        # Validate mixture
        if mixture.shape[-1] != self.n_topics:
            raise ValueError(
                f"Mixture shape {mixture.shape} doesn't match n_topics {self.n_topics}"
            )

        # Project mixture onto topic vectors
        return mixture @ self.vectors

    def save(self, path: Path) -> None:
        """Save topic vectors."""
        # Save to CPU tensors for better compatibility
        torch.save(self.vectors.cpu(), path)

    @classmethod
    def load(cls, path: Path, device: str | None = None) -> "TopicVectorSpace":
        """Load topic vectors and construct space."""
        vectors = torch.load(path)
        n_topics, vocab_size = vectors.shape
        return cls(n_topics, vocab_size, vectors=vectors, device=device)



---
File: faux_lingo/core/transitions.py
---
# faux_lingo/core/transitions.py
"""Transition probability matrices combining topic and color constraints."""

from typing import TypeAlias

import torch
from typing_extensions import Self

from .colors import ColorSpace
from .topics import TopicVectorSpace

# Type aliases for dimensions
BatchDim: TypeAlias = int
VocabSize: TypeAlias = int


class TransitionMatrix:
    """
    Manages transition probability matrices that respect both topic and color constraints.

    Core properties:
    1. Matrices are proper probability distributions (row-wise sum to 1)
    2. Color transitions follow specified weights
    3. Global token distributions reflect topic mixtures
    """

    def __init__(
        self,
        topic_space: TopicVectorSpace,
        color_space: ColorSpace,
        device: str | None = None,
    ):
        """
        Initialize transition matrix generator.

        Args:
            topic_space: Space of topic vectors
            color_space: Color class definitions and rules
            device: Optional compute device, defaults to CPU

        Raises:
            ValueError: If spaces have incompatible dimensions
        """
        if topic_space.vocab_size != color_space.vocab_size:
            raise ValueError(
                f"Vocab size mismatch: topics ({topic_space.vocab_size}) "
                f"!= colors ({color_space.vocab_size})"
            )

        self.device = device if device else "cpu"
        self.topic_space = topic_space
        self.color_space = color_space
        self.vocab_size = topic_space.vocab_size

    def generate(
        self,
        topic_mixture: torch.Tensor,
        temperature: float = 1.0,
        min_prob: float = 1e-6,
    ) -> torch.Tensor:
        """
        Generate transition probability matrix for given topic mixture.

        Args:
            topic_mixture: Mixture weights for topics [batch_size, n_topics]
            temperature: Controls entropy of distributions (higher = more uniform)
            min_prob: Minimum probability for valid transitions

        Returns:
            Transition probability matrix [batch_size, vocab_size, vocab_size]

        Notes:
            1. Output[b,i,j] = P(token_j | token_i) for sequence b
            2. Each row sums to 1 (is a valid probability distribution)
            3. Respects both topic and color constraints
        """
        # Get base distributions from topics
        base_probs = self.topic_space.get_distribution(topic_mixture)

        # Convert to transition matrix
        # Each row i is the topic distribution masked by valid transitions from token i
        # Expand base probabilities to transition matrix shape
        transitions = base_probs.unsqueeze(1).expand(-1, self.vocab_size, -1)

        # Apply color mask to enforce transition constraints
        color_mask = self.color_space.get_transition_mask()
        transitions = transitions * color_mask

        # Set minimum probability for valid transitions
        transitions = torch.where(
            color_mask > 0,
            torch.maximum(transitions, torch.tensor(min_prob, device=self.device)),
            transitions,
        )

        # Apply temperature scaling to logits before normalization
        if temperature != 1.0:
            transitions = transitions / temperature

        # Normalize each row to get proper probability distributions
        # Small epsilon to avoid division by zero
        row_sums = transitions.sum(dim=-1, keepdim=True) + 1e-10
        transitions = transitions / row_sums

        return transitions

    @classmethod
    def create_uniform(
        cls,
        vocab_size: int,
        n_topics: int,
        color_fractions: list[float],
        device: str | None = None,
    ) -> Self:
        """
        Create transition matrix with uniform topic vectors and color transitions.

        Args:
            vocab_size: Size of token vocabulary
            n_topics: Number of topics to use
            color_fractions: Relative sizes of color classes
            device: Optional compute device

        Returns:
            TransitionMatrix instance with uniform parameters
        """
        topic_space = TopicVectorSpace(
            n_topics=n_topics,
            vocab_size=vocab_size,
            device=device,
        )
        color_space = ColorSpace(
            color_fractions=color_fractions,
            vocab_size=vocab_size,
            device=device,
        )
        return cls(topic_space, color_space, device=device)

    def save(self, path: str) -> None:
        """Save transition parameters."""
        raise NotImplementedError("Saving not yet implemented")

    @classmethod
    def load(cls, path: str, device: str | None = None) -> Self:
        """Load transition parameters."""
        raise NotImplementedError("Loading not yet implemented")



---
File: faux_lingo/core/vocab_builder.py
---
# faux_lingo/core/vocab_builder.py
"""Builder for constructing hierarchical vocabularies."""

import random
from dataclasses import dataclass
from typing import TypeAlias

from loguru import logger

from .vocab_mapping import VocabHierarchy, VocabLevel

# Type aliases
TokenIdx: TypeAlias = int
TokenSeq: TypeAlias = tuple[int, ...]


@dataclass
class BuilderConfig:
    """Configuration for vocabulary hierarchy construction.

    Attributes:
        token_vocab_size: Size of base token vocabulary
        sequence_lengths: List of sequence lengths for each level
        vocab_sizes: List of vocabulary sizes for each level
        seed: Optional random seed for reproducibility
    """

    token_vocab_size: int
    sequence_lengths: list[int]  # Each level's sequence length
    vocab_sizes: list[int]  # Each level's vocabulary size
    seed: int | None = None

    def __post_init__(self) -> None:
        """Validate configuration."""
        if self.token_vocab_size < 1:
            raise ValueError("token_vocab_size must be positive")
        if len(self.sequence_lengths) != len(self.vocab_sizes):
            raise ValueError(
                "Must specify sequence length and vocabulary size for each level"
            )
        if any(length < 1 for length in self.sequence_lengths):
            raise ValueError("All sequence lengths must be positive")
        if any(v < 1 for v in self.vocab_sizes):
            raise ValueError("All vocabulary sizes must be positive")

        # Compute and validate potential combinations at each level
        tokens = self.token_vocab_size
        for level, (length, size) in enumerate(
            zip(self.sequence_lengths, self.vocab_sizes)
        ):
            max_combinations = tokens**length
            if size > max_combinations:
                raise ValueError(
                    f"Level {level}: vocab_size ({size}) exceeds maximum "
                    f"possible combinations ({max_combinations})"
                )
            tokens = size  # Next level builds from this vocabulary


class VocabBuilder:
    """Builds hierarchical vocabularies with constrained structure.

    Core functionality:
    1. Random sampling of valid token sequences
    2. Building vocabularies level by level
    3. Tracking used sequences to avoid duplicates
    """

    def __init__(self, config: BuilderConfig):
        """Initialize builder with configuration.

        Args:
            config: Parameters for vocabulary construction
        """
        self.config = config
        self._rng = random.Random(config.seed)

        # Initialize sequence tracking
        self._used_sequences: list[set[TokenSeq]] = [
            set() for _ in range(len(config.vocab_sizes))
        ]

        logger.info("Initialized VocabBuilder with config: {}", config)

    def build(self) -> VocabHierarchy:
        """Build complete vocabulary hierarchy.

        Returns:
            VocabHierarchy with all levels constructed
        """
        levels: list[VocabLevel] = []
        current_vocab_size = self.config.token_vocab_size

        # Build each level
        for level, (seq_len, vocab_size) in enumerate(
            zip(self.config.sequence_lengths, self.config.vocab_sizes)
        ):
            logger.debug("Building level {} vocabulary...", level)

            # Generate valid sequences for this level
            sequences: dict[TokenIdx, TokenSeq] = {}
            while len(sequences) < vocab_size:
                seq = tuple(
                    self._rng.randrange(current_vocab_size) for _ in range(seq_len)
                )
                if seq not in self._used_sequences[level]:
                    token_idx = len(sequences)
                    sequences[token_idx] = seq
                    self._used_sequences[level].add(seq)

            # Create vocabulary level
            vocab_level = VocabLevel(
                vocab_size=vocab_size,
                chunk_size=seq_len,
                sequences=sequences,
            )
            levels.append(vocab_level)

            # Update for next level
            current_vocab_size = vocab_size

        return VocabHierarchy(levels)

    @classmethod
    def create_default_config(cls) -> BuilderConfig:
        """Create configuration with reasonable defaults.

        Returns:
            BuilderConfig for simple three-level hierarchy
        """
        return BuilderConfig(
            token_vocab_size=10,  # Base tokens (0-9)
            sequence_lengths=[2, 3, 2],  # Length at each level
            vocab_sizes=[20, 15, 10],  # Vocabulary sizes
        )


def create_word_hierarchy(
    token_vocab_size: int = 10,
    n_chars: int = 20,
    n_words: int = 100,
    chars_per_word: int = 3,
    seed: int | None = None,
) -> VocabHierarchy:
    """Convenience function to create character-word vocabulary.

    Args:
        token_vocab_size: Size of base token vocabulary
        n_chars: Number of unique characters
        n_words: Number of unique words
        chars_per_word: Number of characters per word
        seed: Optional random seed

    Returns:
        Two-level hierarchy mapping words to character sequences
    """
    config = BuilderConfig(
        token_vocab_size=token_vocab_size,
        sequence_lengths=[2, chars_per_word],  # tokens->chars, chars->words
        vocab_sizes=[n_chars, n_words],
        seed=seed,
    )
    return VocabBuilder(config).build()



---
File: faux_lingo/core/vocab_extensions.py
---
# faux_lingo/core/vocab_extensions.py
"""Extensions to vocabulary system for multiple mappings and augmentations."""

from dataclasses import dataclass
from typing import Sequence, TypeAlias

import torch

from .vocab_mapping import TokenIdx, TokenSeq, VocabHierarchy

# Type aliases
Probability: TypeAlias = float
AugmentedSeq: TypeAlias = tuple[TokenSeq, Probability]


@dataclass
class MultiMappingLevel:
    """Vocabulary level supporting multiple mappings.

    Attributes:
        vocab_size: Number of tokens at this level
        chunk_size: Number of tokens from parent level per token
        sequences: Mapping of token to list of possible sequences with probabilities
    """

    vocab_size: int
    chunk_size: int
    sequences: dict[TokenIdx, list[AugmentedSeq]]

    def __post_init__(self) -> None:
        """Validate level properties."""
        if self.vocab_size < 1:
            raise ValueError("vocab_size must be positive")
        if self.chunk_size < 1:
            raise ValueError("chunk_size must be positive")
        if len(self.sequences) != self.vocab_size:
            raise ValueError(
                f"Number of sequences ({len(self.sequences)}) "
                f"!= vocab_size ({self.vocab_size})"
            )

        # Validate sequence probabilities
        for token, seqs in self.sequences.items():
            if not seqs:
                raise ValueError(f"No sequences defined for token {token}")
            probs = [prob for _, prob in seqs]
            if not torch.allclose(torch.tensor(sum(probs)), torch.tensor(1.0)):
                raise ValueError(
                    f"Sequence probabilities for token {token} do not sum to 1"
                )


class MultiMappingHierarchy:
    """Hierarchical vocabulary with multiple possible mappings.

    Core functionality:
    1. Support for multiple sequences mapping to same token
    2. Probabilistic sequence selection during decoding
    3. Integration with existing vocabulary system
    """

    def __init__(
        self,
        levels: Sequence[MultiMappingLevel],
        device: str | None = None,
    ):
        """Initialize hierarchy with multiple mapping levels.

        Args:
            levels: Sequence of vocabulary levels from lowest to highest
            device: Optional compute device, defaults to CPU
        """
        self.device = device if device else "cpu"
        self.levels = list(levels)

    def decode_sequence(
        self,
        tokens: torch.Tensor,
        start_level: int,
        target_level: int,
        seed: int | None = None,
    ) -> torch.Tensor:
        """Decode token sequence with probabilistic mapping selection.

        Args:
            tokens: Input token sequence [batch_size, seq_len]
            start_level: Index of starting vocabulary level
            target_level: Index of target vocabulary level
            seed: Optional random seed for reproducible decoding

        Returns:
            Decoded token sequences at target level [batch_size, new_seq_len]
        """
        if seed is not None:
            torch.manual_seed(seed)

        if not (0 <= start_level < len(self.levels)):
            raise ValueError(f"Invalid start_level: {start_level}")
        if not (0 <= target_level < len(self.levels)):
            raise ValueError(f"Invalid target_level: {target_level}")
        if target_level > start_level:
            raise ValueError("Can only decode to same or lower levels")

        # Return input tokens if no decoding needed
        if target_level == start_level:
            return tokens

        # Start with input tokens
        current = tokens

        # Decode through intermediate levels
        for level in range(start_level - 1, target_level - 1, -1):
            # Get all possible sequences for each token
            level_seqs = self.levels[level].sequences
            max_seq_len = max(
                len(seq) for seqs in level_seqs.values() for seq, _ in seqs
            )

            # Create output tensor with padding
            result = torch.full(
                (
                    current.shape[0],
                    current.shape[1] * max_seq_len,
                ),
                -1,
                dtype=torch.long,
                device=self.device,
            )

            # Process each token in batch
            for b in range(current.shape[0]):
                pos = 0
                for t in range(current.shape[1]):
                    # Ensure integer token index
                    token_idx = int(current[b, t].item())
                    if token_idx == -1:  # Skip padding
                        continue

                    # Get possible sequences and probabilities
                    seqs = level_seqs[token_idx]
                    probs = torch.tensor([p for _, p in seqs], device=self.device)

                    # Sample sequence based on probabilities and convert to int
                    seq_idx = int(torch.multinomial(probs, 1).item())
                    seq, _ = seqs[seq_idx]

                    # Add sequence to result
                    result[b, pos : pos + len(seq)] = torch.tensor(
                        seq, device=self.device
                    )
                    pos += len(seq)

            current = result

        return current


@dataclass
class AugmentationConfig:
    """Configuration for sequence augmentation.

    Attributes:
        deletion_prob: Probability of character deletion
        insertion_prob: Probability of random character insertion
        substitution_prob: Probability of character substitution
        transposition_prob: Probability of adjacent character transposition
        seed: Optional random seed for reproducibility
    """

    deletion_prob: float = 0.05
    insertion_prob: float = 0.05
    substitution_prob: float = 0.05
    transposition_prob: float = 0.05
    seed: int | None = None

    def __post_init__(self) -> None:
        """Validate configuration."""
        probs = [
            self.deletion_prob,
            self.insertion_prob,
            self.substitution_prob,
            self.transposition_prob,
        ]
        if any(p < 0 or p > 1 for p in probs):
            raise ValueError("All probabilities must be between 0 and 1")
        if sum(probs) > 1:
            raise ValueError("Sum of probabilities must not exceed 1")


class SequenceAugmenter:
    """Applies random perturbations to token sequences.

    Core functionality:
    1. Character-level augmentations (deletion, insertion, etc.)
    2. Controlled randomization based on probabilities
    3. Vocabulary-aware modifications
    """

    def __init__(
        self,
        vocab_size: int,
        config: AugmentationConfig,
        device: str | None = None,
    ):
        """Initialize augmenter with vocabulary and configuration.

        Args:
            vocab_size: Size of token vocabulary
            config: Augmentation parameters
            device: Optional compute device, defaults to CPU
        """
        self.vocab_size = vocab_size
        self.config = config
        self.device = device if device else "cpu"

        if config.seed is not None:
            torch.manual_seed(config.seed)

    def augment_sequence(self, sequence: TokenSeq) -> TokenSeq:
        """Apply random augmentations to token sequence.

        Args:
            sequence: Input token sequence

        Returns:
            Augmented token sequence
        """
        seq = list(sequence)

        # Apply augmentations in random order
        ops = [
            (self._delete, self.config.deletion_prob),
            (self._insert, self.config.insertion_prob),
            (self._substitute, self.config.substitution_prob),
            (self._transpose, self.config.transposition_prob),
        ]

        for op, prob in ops:
            if torch.rand(1).item() < prob:
                seq = op(seq)

        return tuple(seq)

    def _delete(self, seq: list[int]) -> list[int]:
        """Randomly delete a token."""
        if len(seq) <= 1:
            return seq
        idx = int(torch.randint(len(seq), (1,)).item())
        return seq[:idx] + seq[idx + 1 :]

    def _insert(self, seq: list[int]) -> list[int]:
        """Insert random token."""
        idx = int(torch.randint(len(seq) + 1, (1,)).item())
        token = int(torch.randint(self.vocab_size, (1,)).item())
        result = seq.copy()
        result.insert(idx, token)
        return result

    def _substitute(self, seq: list[int]) -> list[int]:
        """Replace token with a different random token."""
        if not seq:
            return seq
        idx = int(torch.randint(len(seq), (1,)).item())
        current = seq[idx]
        # Generate new token until it's different from current
        while True:
            token = int(torch.randint(self.vocab_size, (1,)).item())
            if token != current:
                break
        result = seq.copy()
        result[idx] = token
        return result

    def _transpose(self, seq: list[int]) -> list[int]:
        """Swap adjacent tokens."""
        if len(seq) <= 1:
            return seq
        idx = int(torch.randint(len(seq) - 1, (1,)).item())
        result = seq.copy()
        result[idx], result[idx + 1] = result[idx + 1], result[idx]
        return result


def convert_to_multi_mapping(
    hierarchy: VocabHierarchy,
    augmenter: SequenceAugmenter | None = None,
    n_variants: int = 3,
) -> MultiMappingHierarchy:
    """Convert standard hierarchy to multi-mapping hierarchy.

    Args:
        hierarchy: Standard vocabulary hierarchy
        augmenter: Optional sequence augmenter for variants
        n_variants: Number of variants to generate per sequence

    Returns:
        MultiMappingHierarchy with original and variant sequences
    """
    levels = []

    for level in hierarchy:
        multi_sequences: dict[TokenIdx, list[AugmentedSeq]] = {}

        for token, base_seq in level.sequences.items():
            variants: list[AugmentedSeq] = [
                (base_seq, 0.6)
            ]  # Original sequence gets higher weight

            if augmenter:
                # Generate variants with augmentation
                n_aug = min(n_variants - 1, 5)  # Cap number of variants
                prob_per_variant = (1.0 - 0.6) / n_aug

                for _ in range(n_aug):
                    variant = augmenter.augment_sequence(base_seq)
                    variants.append((variant, prob_per_variant))

            multi_sequences[token] = variants

        multi_level = MultiMappingLevel(
            vocab_size=level.vocab_size,
            chunk_size=level.chunk_size,
            sequences=multi_sequences,
        )
        levels.append(multi_level)

    return MultiMappingHierarchy(levels)



---
File: faux_lingo/core/vocab_mapping.py
---
# faux_lingo/core/vocab_mapping.py
"""Hierarchical vocabulary mapping and decoding."""

from dataclasses import dataclass
from typing import Iterator, Sequence, TypeAlias

import torch
from typing_extensions import Self

# Type aliases for clarity
TokenIdx: TypeAlias = int
TokenSeq: TypeAlias = tuple[int, ...]
Shape: TypeAlias = tuple[int, ...]


@dataclass
class VocabLevel:
    """
    A single level in the vocabulary hierarchy.
        Attributes:
        vocab_size: Number of tokens at this level
        chunk_size: Number of tokens from parent level per token
        sequences: Mapping of each token to its constituent sequence
    """

    vocab_size: int
    chunk_size: int
    sequences: dict[TokenIdx, TokenSeq]

    def __post_init__(self) -> None:
        """Validate vocabulary level properties."""
        if self.vocab_size < 1:
            raise ValueError("vocab_size must be positive")
        if self.chunk_size < 1:
            raise ValueError("chunk_size must be positive")
        if len(self.sequences) != self.vocab_size:
            raise ValueError(
                f"Number of sequences ({len(self.sequences)}) "
                f"!= vocab_size ({self.vocab_size})"
            )
        # Validate all sequences are proper tuples of integers
        for token, seq in self.sequences.items():
            if not isinstance(seq, tuple):
                raise ValueError(f"Sequence for token {token} must be a tuple")
            if not all(isinstance(t, int) for t in seq):
                raise ValueError(
                    f"All elements in sequence for token {token} must be integers"
                )

    @property
    def max_sequence_length(self) -> int:
        """Maximum length of any sequence in this level."""
        return max(len(seq) for seq in self.sequences.values())


class VocabHierarchy:
    """
    Manages hierarchical relationships between vocabulary levels.

    Note: VocabLevels represent mappings BETWEEN levels, not the levels themselves.
    With n VocabLevels, we actually have n+1 vocabulary levels total.
    Level indexing goes from most abstract (0) to most concrete (n):
    Level 0 -> Level 1 (Mapping A)
    Level 1 -> Level 2 (Mapping B)
    """

    def __init__(
        self,
        levels: Sequence[VocabLevel],
        device: str | None = None,
    ) -> None:
        """Initialize vocabulary hierarchy.

        Args:
            levels: Sequence of vocabulary mappings from highest to lowest abstraction
            device: Optional compute device, defaults to CPU
        """
        self.device = device if device else "cpu"
        self.levels = list(levels)
        self.num_levels = len(self.levels) + 1
        self.decode_tables = self._build_decode_tables()

    def decode_sequence(
        self,
        tokens: torch.Tensor,
        start_level: int | None = None,
        target_level: int | None = None,
    ) -> torch.Tensor:
        """Decode token sequence from one level to another.

        Args:
            tokens: Input token sequence [batch_size, seq_len]
            start_level: Optional starting level (defaults to 0)
            target_level: Optional target level (defaults to max level)
        Returns:
            Decoded token sequences at target level [batch_size, new_seq_len]
        """
        # Default to decoding from top level to bottom level
        if start_level is None:
            start_level = 0
        if target_level is None:
            target_level = self.num_levels - 1

        if not (0 <= start_level < self.num_levels):
            raise ValueError(f"Invalid start_level: {start_level}")
        if not (0 <= target_level < self.num_levels):
            raise ValueError(f"Invalid target_level: {target_level}")
        if target_level < start_level:
            raise ValueError("Can only decode to same or higher levels")

        # Return input tokens if no decoding needed
        if target_level == start_level:
            return tokens

        # Start with input tokens
        current = tokens

        # Decode through intermediate levels
        for level in range(start_level, target_level):
            table = self.decode_tables[level]
            decoded = table[current]

            # Remove padding and flatten sequence
            mask = decoded != -1
            lengths = mask.sum(dim=-1)
            max_length = int(lengths.sum(dim=-1).max().item())

            # Create output tensor with proper shape and type
            result = torch.full(
                size=(decoded.shape[0], max_length),
                fill_value=-1,
                dtype=torch.long,
                device=self.device,
            )

            # Fill in decoded sequences
            pos = 0
            for i in range(decoded.shape[1]):
                seq_lengths = lengths[:, i]
                for b in range(decoded.shape[0]):
                    length = int(seq_lengths[b].item())
                    if length > 0:
                        result[b, pos : pos + length] = decoded[b, i, :length]
                pos += int(seq_lengths.max().item())

            current = result

        return current[current != -1].view(tokens.shape[0], -1)

    def _build_decode_tables(self) -> list[torch.Tensor]:
        """Build lookup tables for decoding between levels.

        Returns:
            List of tensors mapping level i tokens to level i+1 sequences
            Each tensor has shape [parent_vocab_size, max_child_sequence_length]
            with padded sequences for consistent shape
        """
        tables = []
        for level in self.levels:
            max_length = max(len(seq) for seq in level.sequences.values())

            table = torch.full(
                size=(level.vocab_size, max_length),
                fill_value=-1,
                dtype=torch.long,
                device=self.device,
            )

            for token, sequence in level.sequences.items():
                table[token, : len(sequence)] = torch.tensor(
                    sequence, dtype=torch.long, device=self.device
                )

            tables.append(table)

        return tables

    @classmethod
    def from_sequences(
        cls,
        sequences: list[dict[TokenIdx, TokenSeq]],
        chunk_sizes: list[int],
        device: str | None = None,
    ) -> Self:
        """Create hierarchy from sequence mappings.
        Args:
            sequences: Mappings for each level
            chunk_sizes: Number of tokens per chunk at each level
            device: Optional compute device
        Returns:
            Initialized VocabHierarchy
        """
        if len(sequences) != len(chunk_sizes):
            raise ValueError("Must provide chunk size for each level")

        levels = []
        for seq_map, chunk_size in zip(sequences, chunk_sizes):
            level = VocabLevel(
                vocab_size=len(seq_map),
                chunk_size=chunk_size,
                sequences=seq_map,
            )
            levels.append(level)

        return cls(levels, device=device)

    def __getitem__(self, level: int) -> VocabLevel:
        """Get vocabulary level by index."""
        return self.levels[level]

    def __len__(self) -> int:
        """Get number of vocabulary levels."""
        return len(self.levels)

    def __iter__(self) -> Iterator[VocabLevel]:
        """Iterate over vocabulary levels."""
        return iter(self.levels)



---
File: faux_lingo/data/dataset.py
---
# faux_lingo/data/dataset.py
"""Dataset management for sequence generation."""

from dataclasses import dataclass
from typing import Iterator, TypeAlias, TypedDict

import torch

from ..core.generator import GeneratedSequences, SequenceGenerator

# Type aliases for dimensions
BatchDim: TypeAlias = int
SeqLen: TypeAlias = int


class BatchStats(TypedDict):
    mean_log_prob: float
    topic_weights: list[float]
    color_counts: list[int]


@dataclass
class DatasetConfig:
    """Configuration for dataset generation.

    Attributes:
        batch_size: Number of sequences per batch
        seq_length: Length of each sequence
        n_batches: Total number of batches to generate
        temperature: Controls randomness in generation
        seed: Random seed for reproducibility
    """

    batch_size: int
    seq_length: int
    n_batches: int
    temperature: float = 1.0
    seed: int | None = None


class SequenceDataset:
    """Manages generation and iteration of sequence batches.

    Core functionality:
    1. Batch generation with consistent configuration
    2. Tracking of sequence properties and metadata
    3. Iterator interface for training/validation
    """

    def __init__(
        self,
        generator: SequenceGenerator,
        config: DatasetConfig,
    ):
        """Initialize dataset with generator and configuration.

        Args:
            generator: Sequence generator instance
            config: Dataset generation parameters
        """
        self.generator = generator
        self.config = config
        self.device = generator.device

        # Set random seed if provided
        if config.seed is not None:
            torch.manual_seed(config.seed)

        # Initialize dataset properties
        self.total_sequences = config.batch_size * config.n_batches
        self._current_batch = 0
        self._cached_batch: GeneratedSequences | None = None

    def __len__(self) -> int:
        """Get total number of batches."""
        return self.config.n_batches

    def __iter__(self) -> Iterator[GeneratedSequences]:
        """Create iterator over sequence batches."""
        return self

    def __next__(self) -> GeneratedSequences:
        """Get next batch of sequences."""
        if self._current_batch >= self.config.n_batches:
            self._current_batch = 0
            raise StopIteration

        self._current_batch += 1
        return self.generate_batch()

    def generate_batch(
        self,
        topic_mixtures: torch.Tensor | None = None,
        start_color: int | None = None,
    ) -> GeneratedSequences:
        """Generate a single batch of sequences.

        Args:
            topic_mixtures: Optional pre-specified topic mixtures
            start_color: Optional color index to start sequences with

        Returns:
            GeneratedSequences containing tokens and properties
        """
        if start_color is not None:
            sequences = self.generator.generate_with_color(
                batch_size=self.config.batch_size,
                seq_length=self.config.seq_length,
                start_color=start_color,
                temperature=self.config.temperature,
                topic_mixtures=topic_mixtures,
            )
        else:
            sequences = self.generator.generate(
                batch_size=self.config.batch_size,
                seq_length=self.config.seq_length,
                temperature=self.config.temperature,
                topic_mixtures=topic_mixtures,
            )

        self._cached_batch = sequences
        return sequences

    def get_color_sequences(self, tokens: torch.Tensor) -> torch.Tensor:
        """Convert token sequences to color sequences.

        Args:
            tokens: Token sequences [batch_size, seq_length]

        Returns:
            Color sequences [batch_size, seq_length]
        """
        return torch.tensor(
            [
                [
                    self.generator.transition_model.color_space.get_color(idx.item())
                    for idx in seq
                ]
                for seq in tokens
            ],
            device=self.device,
            dtype=torch.long,
        )

    def get_batch_stats(self, batch: GeneratedSequences) -> BatchStats:
        """Compute statistics for a batch of sequences.

        Args:
            batch: Batch of generated sequences

        Returns:
            Dictionary of batch statistics
        """
        color_seqs = self.get_color_sequences(batch.tokens)

        stats: BatchStats = {
            "mean_log_prob": batch.log_probs.mean().item(),
            "topic_weights": batch.topic_mixtures.mean(0).tolist(),
            "color_counts": torch.bincount(
                color_seqs.view(-1),
                minlength=self.generator.transition_model.color_space.n_colors,
            ).tolist(),
        }

        return stats

    @property
    def vocab_size(self) -> int:
        """Get vocabulary size of generator."""
        return self.generator.vocab_size

    @property
    def n_topics(self) -> int:
        """Get number of topics in generator."""
        return self.generator.transition_model.topic_space.n_topics

    @property
    def n_colors(self) -> int:
        """Get number of color classes in generator."""
        return self.generator.transition_model.color_space.n_colors



---
File: pyproject.toml
---
[build-system]
requires = ["setuptools>=42", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "faux-lingo"
version = "0.1.0"
description = "Research tool for generating synthetic datasets with parameterized information entropy and structural complexity"
readme = "README.md"
requires-python = ">=3.12"
license = { file = "LICENSE" }
authors = [
    { name = "David Marx", email = "david.marx@gmail.com" }
]
classifiers = [
    "Development Status :: 3 - Alpha",
    "Intended Audience :: Science/Research",
    "License :: OSI Approved :: MIT License",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.12",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
]
dependencies = [
    "numpy>=1.21.0",
    "loguru>=0.7.0",
    "torch>=2.0.0",
    "jaxtyping",
    "tensorizer",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.0.0",
    "pytest-cov>=4.0.0",
    "black>=23.0.0",
    "isort>=5.0.0",
    "mypy>=1.0.0",
    "ruff>=0.1.0",
    "types-PyYAML",
    "types-setuptools",
]

[tool.pytest.ini_options]
minversion = "7.0"
addopts = "-ra -q --cov=faux_lingo"
testpaths = ["tests"]

[tool.black]
line-length = 88
target-version = ['py312']
include = '\.pyi?$'

[tool.isort]
profile = "black"
multi_line_output = 3

[tool.mypy]
python_version = "3.12"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true
strict_equality = true  # Added for better equality checks
allow_redefinition = false

# For numpy/torch compatibility
ignore_missing_imports = true
follow_imports = "normal"
no_implicit_reexport = true

# Additional error codes to enable
enable_error_code = ["truthy-bool", "redundant-expr", "ignore-without-code"]

# For better error messages
pretty = true

[tool.setuptools.packages.find]
exclude = ["attic*"]



---
File: tests/test_colors.py
---
# faux_lingo/tests/test_colors.py
"""Tests for color space functionality."""

import pytest
import torch

from faux_lingo.core.colors import ColorSpace


def test_normalization():
    """Test that color fractions are properly normalized."""
    # Test with list input
    space = ColorSpace(color_fractions=[3, 2, 1], vocab_size=60)
    expected = torch.tensor([0.5, 0.333333, 0.166667], dtype=torch.float32)
    assert torch.allclose(space.mapping.fractions, expected, rtol=1e-5)

    # Test with tensor input
    space = ColorSpace(color_fractions=torch.tensor([1.0, 2.0, 3.0]), vocab_size=60)
    expected = torch.tensor([0.166667, 0.333333, 0.5], dtype=torch.float32)
    assert torch.allclose(space.mapping.fractions, expected, rtol=1e-5)


def test_boundaries():
    """Test token boundary calculations."""
    space = ColorSpace(color_fractions=[1, 1, 1], vocab_size=100)

    # Check boundary points
    assert space.get_color_range(0) == (0, 33)  # First third
    assert space.get_color_range(1) == (33, 66)  # Second third
    assert space.get_color_range(2) == (66, 100)  # Last third (gets remainder)

    # Test boundary adjustments
    space = ColorSpace(color_fractions=[1, 1], vocab_size=7)
    assert space.get_color_range(1)[1] == 7  # Last range should end at vocab_size


def test_color_lookup():
    """Test token to color mapping."""
    space = ColorSpace(color_fractions=[1, 2, 1], vocab_size=40)

    # With fractions [1,2,1], boundaries should be at [0, 10, 30, 40]
    # Print actual boundaries for debugging
    print(f"Boundaries: {space.mapping.boundaries}")
    print(f"Color of token 10: {space.get_color(10)}")
    print(f"Fractions: {space.mapping.fractions}")

    # Test boundaries
    assert space.get_color(0) == 0  # First color
    assert space.get_color(10) == 1  # Middle color
    assert space.get_color(39) == 2  # Last color

    # Test invalid indices
    with pytest.raises(ValueError):
        space.get_color(-1)
    with pytest.raises(ValueError):
        space.get_color(40)


def test_transition_weights():
    """Test transition weight validation and mask creation."""
    weights = torch.tensor([[1.0, 0.5, 0.0], [0.5, 1.0, 0.5], [0.0, 0.5, 1.0]])

    space = ColorSpace(
        color_fractions=[1, 1, 1], vocab_size=9, transition_weights=weights
    )

    mask = space.get_transition_mask()

    # Check mask shape
    assert mask.shape == (9, 9)

    # Check block structure
    block_size = 3
    for i in range(3):
        for j in range(3):
            block = mask[
                i * block_size : (i + 1) * block_size,
                j * block_size : (j + 1) * block_size,
            ]
            assert torch.all(block == weights[i, j])

    # Test invalid weights
    with pytest.raises(ValueError):
        ColorSpace(
            color_fractions=[1, 1],
            vocab_size=10,
            transition_weights=weights,  # Wrong shape for n_colors=2
        )

    with pytest.raises(ValueError):
        ColorSpace(
            color_fractions=[1, 1],
            vocab_size=10,
            transition_weights=torch.tensor(
                [[-1.0, 1.0], [1.0, -1.0]]
            ),  # Negative weights
        )


def test_save_load(tmp_path):
    """Test serialization of color space."""
    weights = torch.tensor([[1.0, 0.5], [0.5, 1.0]])
    original = ColorSpace(
        color_fractions=[2, 3], vocab_size=100, transition_weights=weights
    )

    path = tmp_path / "color_space.pt"

    # Save and load
    original.save(path)
    loaded = ColorSpace.load(path)

    # Verify properties preserved
    assert original.vocab_size == loaded.vocab_size
    assert torch.allclose(original.mapping.fractions, loaded.mapping.fractions)
    assert torch.allclose(original.mapping.boundaries, loaded.mapping.boundaries)
    assert torch.allclose(original.transition_weights, loaded.transition_weights)


def test_device_handling():
    """Test device placement and movement."""
    # Default to CPU
    space = ColorSpace(color_fractions=[1, 1], vocab_size=10)
    assert space.mapping.boundaries.device.type == "cpu"
    assert space.transition_weights.device.type == "cpu"

    # Test device specification
    space = ColorSpace(color_fractions=[1, 1], vocab_size=10, device="cpu")
    assert space.mapping.boundaries.device.type == "cpu"

    # Test mask device matches space
    mask = space.get_transition_mask()
    assert mask.device.type == "cpu"



---
File: tests/test_dataset.py
---
# faux_lingo/tests/test_dataset.py
"""Tests for dataset generation functionality."""

import pytest
import torch

from faux_lingo.core.generator import SequenceGenerator
from faux_lingo.data.dataset import DatasetConfig, SequenceDataset


@pytest.fixture
def simple_generator():
    """Create a simple generator for testing."""
    return SequenceGenerator.create_uniform(
        vocab_size=9,
        n_topics=2,
        color_fractions=[1, 1, 1],  # Three equal color classes
    )


@pytest.fixture
def simple_config():
    """Create a basic dataset configuration."""
    return DatasetConfig(
        batch_size=4,
        seq_length=10,
        n_batches=3,
        seed=42,
    )


def test_dataset_iteration(simple_generator, simple_config):
    """Test basic dataset iteration."""
    dataset = SequenceDataset(simple_generator, simple_config)

    # Check total length
    assert len(dataset) == simple_config.n_batches

    # Check batch shapes
    batches = list(dataset)
    assert len(batches) == simple_config.n_batches
    for batch in batches:
        assert batch.tokens.shape == (simple_config.batch_size, simple_config.seq_length)
        assert batch.topic_mixtures.shape == (simple_config.batch_size, dataset.n_topics)


def test_reproducibility(simple_generator, simple_config):
    """Test that sequence generation is reproducible with same seed."""
    dataset1 = SequenceDataset(simple_generator, simple_config)
    batch1 = next(iter(dataset1))

    dataset2 = SequenceDataset(simple_generator, simple_config)
    batch2 = next(iter(dataset2))

    assert torch.all(batch1.tokens == batch2.tokens)
    assert torch.allclose(batch1.log_probs, batch2.log_probs)
    assert torch.allclose(batch1.topic_mixtures, batch2.topic_mixtures)


def test_color_sequence_conversion(simple_generator, simple_config):
    """Test conversion of tokens to color sequences."""
    dataset = SequenceDataset(simple_generator, simple_config)
    batch = next(iter(dataset))

    color_seqs = dataset.get_color_sequences(batch.tokens)
    assert color_seqs.shape == batch.tokens.shape

    # Check color indices are valid
    assert torch.all(color_seqs >= 0)
    assert torch.all(color_seqs < dataset.n_colors)


def test_batch_stats(simple_generator, simple_config):
    """Test batch statistics computation."""
    dataset = SequenceDataset(simple_generator, simple_config)
    batch = next(iter(dataset))
    stats = dataset.get_batch_stats(batch)

    # Check required statistics are present
    assert "mean_log_prob" in stats
    assert "topic_weights" in stats
    assert "color_counts" in stats

    # Check shapes and values
    assert len(stats["topic_weights"]) == dataset.n_topics
    assert len(stats["color_counts"]) == dataset.n_colors
    assert sum(stats["color_counts"]) == simple_config.batch_size * simple_config.seq_length


def test_color_constrained_generation(simple_generator, simple_config):
    """Test generation with specific start color."""
    dataset = SequenceDataset(simple_generator, simple_config)
    start_color = 1

    batch = dataset.generate_batch(start_color=start_color)
    color_seqs = dataset.get_color_sequences(batch.tokens)

    # Check first token of each sequence is correct color
    assert torch.all(color_seqs[:, 0] == start_color)


def test_topic_constrained_generation(simple_generator, simple_config):
    """Test generation with specific topic mixtures."""
    dataset = SequenceDataset(simple_generator, simple_config)
    
    # Create specific topic mixture
    mixtures = torch.tensor([
        [0.8, 0.2],  # Strong bias to first topic
        [0.2, 0.8],  # Strong bias to second topic
        [0.5, 0.5],  # Equal mixture
        [1.0, 0.0],  # Pure first topic
    ])

    batch = dataset.generate_batch(topic_mixtures=mixtures)
    assert torch.allclose(batch.topic_mixtures, mixtures)


def test_device_handling(simple_generator, simple_config):
    """Test device placement and consistency."""
    dataset = SequenceDataset(simple_generator, simple_config)
    batch = next(iter(dataset))

    # Check all tensors are on same device
    assert batch.tokens.device.type == dataset.device
    assert batch.topic_mixtures.device.type == dataset.device
    assert batch.log_probs.device.type == dataset.device

    # Test color sequence conversion maintains device
    color_seqs = dataset.get_color_sequences(batch.tokens)
    assert color_seqs.device.type == dataset.device



---
File: tests/test_entropy.py
---
# faux_lingo/tests/test_entropy.py
"""Tests for entropy analysis functionality."""

import pytest
import torch

from faux_lingo.analysis.entropy import EntropyAnalyzer, EntropyMetrics
from faux_lingo.core.generator import GeneratedSequences, SequenceGenerator


@pytest.fixture
def simple_analyzer():
    """Create analyzer with simple uniform generator."""
    generator = SequenceGenerator.create_uniform(
        vocab_size=9,
        n_topics=2,
        color_fractions=[1, 1, 1],  # Three equal color classes
    )
    return EntropyAnalyzer(generator.transition_model)


@pytest.fixture
def sample_sequences(simple_analyzer):
    """Generate sample sequences for testing."""
    generator = SequenceGenerator(simple_analyzer.transition_model)
    return generator.generate(batch_size=10, seq_length=20)


def test_metrics_zero():
    """Test zero initialization of metrics."""
    metrics = EntropyMetrics.zero()
    assert metrics.color_entropy == 0.0
    assert metrics.topic_entropy == 0.0
    assert metrics.token_entropy == 0.0


def test_color_entropy(simple_analyzer):
    """Test color entropy computation with different transition rules."""
    # Generate sequences with uniform transitions
    generator = SequenceGenerator(simple_analyzer.transition_model)
    uniform_sequences = generator.generate(batch_size=10, seq_length=20)
    uniform_metrics = simple_analyzer.analyze_sequences(uniform_sequences)

    # Generate sequences with deterministic transitions
    det_weights = torch.eye(3)  # Only self-transitions allowed
    simple_analyzer.transition_model.color_space.transition_weights = det_weights
    det_generator = SequenceGenerator(simple_analyzer.transition_model)
    det_sequences = det_generator.generate(batch_size=10, seq_length=20)
    det_metrics = simple_analyzer.analyze_sequences(det_sequences)

    # Deterministic transitions should have lower entropy
    assert det_metrics.color_entropy < uniform_metrics.color_entropy

    # Entropy should be non-negative and bounded
    assert det_metrics.color_entropy >= 0
    max_entropy = torch.log2(torch.tensor(3.0))  # log2(num_colors)
    assert det_metrics.color_entropy <= max_entropy


def test_topic_entropy(simple_analyzer):
    """Test topic entropy computation."""
    # Test with uniform mixture
    uniform_mix = torch.ones(4, 2) / 2
    sequences = GeneratedSequences(
        tokens=torch.zeros(4, 10, dtype=torch.long),  # Dummy tokens
        topic_mixtures=uniform_mix,
        log_probs=torch.zeros(4),
    )

    metrics = simple_analyzer.analyze_sequences(sequences)
    expected = torch.log2(torch.tensor(2.0))  # log2(num_topics)
    assert torch.isclose(torch.tensor(metrics.topic_entropy), expected, atol=1e-6)

    # Test with deterministic mixture
    det_mix = torch.zeros(4, 2)
    det_mix[:, 0] = 1.0  # All weight on first topic
    sequences.topic_mixtures = det_mix

    metrics = simple_analyzer.analyze_sequences(sequences)
    assert metrics.topic_entropy == 0.0


def test_token_entropy(simple_analyzer, sample_sequences):
    """Test token entropy computation."""
    metrics = simple_analyzer.analyze_sequences(sample_sequences)

    # Entropy should be non-negative and bounded
    assert metrics.token_entropy >= 0
    max_entropy = torch.log2(torch.tensor(9.0))  # log2(vocab_size)
    assert metrics.token_entropy <= max_entropy

    # Test with repeated tokens
    repeated = torch.zeros_like(sample_sequences.tokens)
    sequences = GeneratedSequences(
        tokens=repeated,
        topic_mixtures=sample_sequences.topic_mixtures,
        log_probs=sample_sequences.log_probs,
    )

    metrics = simple_analyzer.analyze_sequences(sequences)
    assert metrics.token_entropy == 0.0


def test_device_handling(simple_analyzer, sample_sequences):
    """Test device placement and consistency."""
    # All computations should happen on analyzer's device
    metrics = simple_analyzer.analyze_sequences(sample_sequences)

    # Move sequences to different device
    cpu_sequences = GeneratedSequences(
        tokens=sample_sequences.tokens.cpu(),
        topic_mixtures=sample_sequences.topic_mixtures.cpu(),
        log_probs=sample_sequences.log_probs.cpu(),
    )

    # Should still work and give same results
    cpu_metrics = simple_analyzer.analyze_sequences(cpu_sequences)
    assert metrics.color_entropy == cpu_metrics.color_entropy
    assert metrics.topic_entropy == cpu_metrics.topic_entropy
    assert metrics.token_entropy == cpu_metrics.token_entropy



---
File: tests/test_generator.py
---
# faux_lingo/tests/test_generator.py
"""Tests for sequence generation."""

import pytest
import torch

from faux_lingo.core.generator import SequenceGenerator


@pytest.fixture
def simple_generator():
    """Create a simple generator for testing."""
    return SequenceGenerator.create_uniform(
        vocab_size=9,
        n_topics=2,
        color_fractions=[1, 1, 1],  # Three equal color classes
    )


def test_sequence_shapes(simple_generator):
    """Test output shapes from generation."""
    batch_size = 4
    seq_length = 10

    sequences = simple_generator.generate(
        batch_size=batch_size,
        seq_length=seq_length,
    )

    assert sequences.tokens.shape == (batch_size, seq_length)
    assert sequences.topic_mixtures.shape == (batch_size, 2)  # 2 topics
    assert sequences.log_probs.shape == (batch_size,)


def test_token_ranges(simple_generator):
    """Test that generated tokens are within vocabulary."""
    sequences = simple_generator.generate(
        batch_size=10,
        seq_length=20,
    )

    assert torch.all(sequences.tokens >= 0)
    assert torch.all(sequences.tokens < simple_generator.vocab_size)


def test_color_start(simple_generator):
    """Test generation with specific start color."""
    batch_size = 5
    color_idx = 1  # Middle color class

    sequences = simple_generator.generate_with_color(
        batch_size=batch_size,
        seq_length=10,
        start_color=color_idx,
    )

    # Get expected token range for color
    start_idx, end_idx = simple_generator.transition_model.color_space.get_color_range(
        color_idx
    )

    # Check first tokens are in correct range
    first_tokens = sequences.tokens[:, 0]
    assert torch.all(first_tokens >= start_idx)
    assert torch.all(first_tokens < end_idx)


def test_temperature_effect(simple_generator):
    """Test that temperature effect is consistent across runs."""
    batch_size = 100
    seq_length = 20
    n_trials = 3

    entropy_diffs = []  # Store hot - cold entropy differences

    for seed in range(n_trials):
        torch.manual_seed(seed)

        # Generate with different temperatures
        cold_seqs = simple_generator.generate(
            batch_size=batch_size,
            seq_length=seq_length,
            temperature=0.1,
        )
        hot_seqs = simple_generator.generate(
            batch_size=batch_size,
            seq_length=seq_length,
            temperature=10.0,
        )

        # Compare transition statistics
        def get_transition_counts(tokens: torch.Tensor) -> torch.Tensor:
            """Get counts of token-to-token transitions."""
            counts = torch.zeros(
                (simple_generator.vocab_size, simple_generator.vocab_size),
                device=tokens.device,
            )
            for i in range(tokens.shape[0]):  # For each sequence
                for t in range(tokens.shape[1] - 1):  # For each transition
                    curr, next = tokens[i, t], tokens[i, t + 1]
                    counts[curr, next] += 1
            return counts

        # Get transition counts and convert to probabilities
        cold_counts = get_transition_counts(cold_seqs.tokens)
        hot_counts = get_transition_counts(hot_seqs.tokens)

        cold_probs = cold_counts / (cold_counts.sum(-1, keepdim=True) + 1e-10)
        hot_probs = hot_counts / (hot_counts.sum(-1, keepdim=True) + 1e-10)

        # Calculate entropies
        def get_entropy(probs: torch.Tensor) -> float:
            """Calculate average entropy of transition distributions."""
            return -(probs * torch.log(probs + 1e-10)).sum(-1).mean().item()

        cold_entropy = get_entropy(cold_probs)
        hot_entropy = get_entropy(hot_probs)

        print(
            f"Trial {seed}: cold={cold_entropy:.4f}, hot={hot_entropy:.4f}, diff={hot_entropy-cold_entropy:.4f}"
        )
        entropy_diffs.append(hot_entropy - cold_entropy)

    # Check if the effect is consistent
    signs = [diff > 0 for diff in entropy_diffs]
    assert all(signs) or not any(
        signs
    ), "Temperature effect should be consistent across trials"


def test_topic_mixture_validation(simple_generator):
    """Test validation of topic mixture inputs."""
    # Wrong batch size
    bad_mixtures = torch.ones(3, 2) / 2  # 3 sequences when asking for 2

    with pytest.raises(ValueError):
        simple_generator.generate(
            batch_size=2,
            seq_length=10,
            topic_mixtures=bad_mixtures,
        )


def test_start_token_validation(simple_generator):
    """Test validation of start token inputs."""
    # Wrong shape
    bad_tokens = torch.zeros(3)  # 3 tokens when asking for 2 sequences

    with pytest.raises(ValueError):
        simple_generator.generate(
            batch_size=2,
            seq_length=10,
            start_tokens=bad_tokens,
        )


def test_color_validation(simple_generator):
    """Test validation of color inputs."""
    with pytest.raises(ValueError):
        simple_generator.generate_with_color(
            batch_size=2,
            seq_length=10,
            start_color=99,  # Invalid color index
        )


def test_log_probability_consistency(simple_generator):
    """Test that log probabilities are consistent with transitions."""
    # Generate single sequence for simplicity
    batch_size = 1
    seq_length = 5
    temperature = 1.0

    # Generate with specific topic mixture
    mixture = torch.tensor([[0.7, 0.3]], device=simple_generator.device)
    sequences = simple_generator.generate(
        batch_size=batch_size,
        seq_length=seq_length,
        topic_mixtures=mixture,
        temperature=temperature,
    )

    # Get transition matrix
    transitions = simple_generator.transition_model.generate(
        mixture,
        temperature=temperature,
    )

    # Manually compute log probability
    manual_log_prob = 0.0
    for t in range(1, seq_length):
        prev_token = sequences.tokens[0, t - 1]
        curr_token = sequences.tokens[0, t]
        prob = transitions[0, prev_token, curr_token]
        manual_log_prob += torch.log(prob).item()

    assert torch.allclose(
        sequences.log_probs[0],
        torch.tensor(manual_log_prob, device=simple_generator.device),
        rtol=1e-5,
    )


def test_reproducibility(simple_generator):
    """Test that sequences are reproducible with same seed."""
    torch.manual_seed(42)
    seq1 = simple_generator.generate(
        batch_size=2,
        seq_length=10,
    )

    torch.manual_seed(42)
    seq2 = simple_generator.generate(
        batch_size=2,
        seq_length=10,
    )

    assert torch.all(seq1.tokens == seq2.tokens)
    assert torch.allclose(seq1.log_probs, seq2.log_probs)



---
File: tests/test_topics.py
---
# Tests in faux_lingo/tests/test_topics.py
"""Tests for topic vector space functionality."""

from pathlib import Path

import pytest
import torch

from faux_lingo.core.topics import TopicVectorSpace


def test_init_validation():
    """Test input validation during initialization."""
    with pytest.raises(ValueError):
        # n_topics > vocab_size not allowed
        TopicVectorSpace(n_topics=5, vocab_size=3)


def test_vector_properties():
    """Test that topic vectors have required mathematical properties."""
    space = TopicVectorSpace(n_topics=3, vocab_size=5)
    vectors = space.vectors

    # Test unit length
    norms = torch.linalg.norm(vectors, dim=1)
    assert torch.allclose(norms, torch.ones_like(norms))

    # Test orthogonality
    gram = vectors @ vectors.T
    identity = torch.eye(3, device=vectors.device)
    assert torch.allclose(gram, identity, atol=1e-6)


def test_distribution_shape():
    """Test output shape of get_distribution."""
    space = TopicVectorSpace(n_topics=3, vocab_size=5)

    # Single mixture
    mixture = torch.ones(3) / 3  # Uniform mixture
    dist = space.get_distribution(mixture)
    assert dist.shape == (5,)

    # Batch of mixtures
    mixtures = torch.ones(4, 3) / 3  # Batch of uniform mixtures
    dists = space.get_distribution(mixtures)
    assert dists.shape == (4, 5)


def test_save_load(tmp_path):
    """Test serialization of topic vectors."""
    original = TopicVectorSpace(n_topics=3, vocab_size=5)
    path = tmp_path / "topics.pt"

    # Save and load
    original.save(path)
    loaded = TopicVectorSpace.load(path)

    # Verify properties preserved
    assert torch.allclose(original.vectors, loaded.vectors)
    assert original.n_topics == loaded.n_topics
    assert original.vocab_size == loaded.vocab_size


def test_device_handling():
    """Test device placement and movement."""
    # Default to CPU
    space = TopicVectorSpace(n_topics=2, vocab_size=4)
    assert space.vectors.device.type == "cpu"

    # Handle CPU mixtures with CPU space
    mixture = torch.ones(2) / 2
    dist = space.get_distribution(mixture)
    assert dist.device.type == "cpu"

    # Test device specification on load
    path = Path("tmp_topics.pt")
    space.save(path)
    loaded = TopicVectorSpace.load(path, device="cpu")
    assert loaded.vectors.device.type == "cpu"
    path.unlink()  # Cleanup


def test_random_state():
    """Test reproducibility of random initialization."""
    torch.manual_seed(42)
    space1 = TopicVectorSpace(n_topics=2, vocab_size=4)

    torch.manual_seed(42)
    space2 = TopicVectorSpace(n_topics=2, vocab_size=4)

    assert torch.allclose(space1.vectors, space2.vectors)



---
File: tests/test_transitions.py
---
# faux_lingo/tests/test_transitions.py
"""Tests for transition matrix generation."""

import pytest
import torch

from faux_lingo.core.colors import ColorSpace
from faux_lingo.core.topics import TopicVectorSpace
from faux_lingo.core.transitions import TransitionMatrix


@pytest.fixture
def simple_matrix():
    """Create a simple transition matrix for testing."""
    return TransitionMatrix.create_uniform(
        vocab_size=9,
        n_topics=2,
        color_fractions=[1, 1, 1],  # Three equal-sized color classes
    )


def test_initialization():
    """Test constructor validation."""
    # Create spaces with mismatched vocab sizes
    topic_space = TopicVectorSpace(n_topics=2, vocab_size=10)
    color_space = ColorSpace(color_fractions=[1, 1], vocab_size=12)

    # Should raise error
    with pytest.raises(ValueError, match="Vocab size mismatch"):
        TransitionMatrix(topic_space, color_space)


def test_uniform_creation():
    """Test creation of uniform transition matrix."""
    matrix = TransitionMatrix.create_uniform(
        vocab_size=6,
        n_topics=2,
        color_fractions=[1, 1],  # Two equal color classes
    )

    assert matrix.vocab_size == 6
    assert matrix.topic_space.n_topics == 2
    assert matrix.color_space.n_colors == 2


def test_probability_properties(simple_matrix):
    """Test that generated matrices have valid probability properties."""
    # Generate matrix with uniform mixture
    mixture = torch.ones(1, 2) / 2  # Equal mixture of two topics
    transitions = simple_matrix.generate(mixture)

    # Check shape
    assert transitions.shape == (1, 9, 9)

    # Check row sums
    row_sums = transitions.sum(dim=-1)
    assert torch.allclose(row_sums, torch.ones_like(row_sums))

    # Check non-negativity
    assert torch.all(transitions >= 0)


def test_color_constraints(simple_matrix):
    """Test that color transition constraints are respected."""
    # Set up transition weights that forbid some transitions
    weights = torch.tensor(
        [
            [1.0, 0.0, 0.0],  # Color 0 can only transition to itself
            [0.0, 1.0, 0.0],  # Color 1 can only transition to itself
            [0.0, 0.0, 1.0],  # Color 2 can only transition to itself
        ]
    )
    simple_matrix.color_space.transition_weights = weights

    # Generate transitions
    mixture = torch.ones(1, 2) / 2
    transitions = simple_matrix.generate(mixture)

    # Check block structure
    for i in range(3):  # For each color
        for j in range(3):  # For each target color
            if i != j:  # Off-diagonal blocks should be zero
                start_i = i * 3
                end_i = (i + 1) * 3
                start_j = j * 3
                end_j = (j + 1) * 3
                block = transitions[0, start_i:end_i, start_j:end_j]
                assert torch.all(block == 0)


def test_temperature_effect(simple_matrix):
    """Test that temperature affects distribution entropy."""
    mixture = torch.ones(1, 2) / 2

    # Generate with different temperatures
    cold = simple_matrix.generate(mixture, temperature=0.1)
    hot = simple_matrix.generate(mixture, temperature=10.0)

    # Higher temperature should give more uniform distributions
    cold_entropy = -(cold * torch.log(cold + 1e-10)).sum(dim=-1).mean()
    hot_entropy = -(hot * torch.log(hot + 1e-10)).sum(dim=-1).mean()

    assert hot_entropy > cold_entropy


def test_batch_generation(simple_matrix):
    """Test generation of multiple matrices simultaneously."""
    # Create batch of different mixtures
    mixtures = torch.tensor(
        [
            [0.8, 0.2],  # First sequence favors topic 0
            [0.2, 0.8],  # Second sequence favors topic 1
        ]
    )

    transitions = simple_matrix.generate(mixtures)

    # Check batch dimension
    assert transitions.shape == (2, 9, 9)

    # Check each matrix independently sums to 1
    row_sums = transitions.sum(dim=-1)
    assert torch.allclose(row_sums, torch.ones_like(row_sums))


def test_min_probability(simple_matrix):
    """Test that minimum probability is respected for valid transitions."""
    mixture = torch.ones(1, 2) / 2
    min_prob = 1e-4

    transitions = simple_matrix.generate(mixture, min_prob=min_prob)

    # Get color mask to identify valid transitions
    color_mask = simple_matrix.color_space.get_transition_mask()
    valid_transitions = transitions[0][color_mask > 0]

    # Check minimum probability is respected
    assert torch.all(valid_transitions >= min_prob)


def test_device_consistency(simple_matrix):
    """Test that all tensors stay on the same device."""
    mixture = torch.ones(1, 2) / 2
    transitions = simple_matrix.generate(mixture)

    # All tensors should be on the same device
    assert transitions.device == simple_matrix.topic_space.vectors.device
    assert transitions.device == simple_matrix.color_space.mapping.boundaries.device


def test_invalid_mixture_shape(simple_matrix):
    """Test validation of topic mixture shape."""
    # Wrong number of topics
    bad_mixture = torch.ones(1, 3) / 3  # 3 topics when space has 2

    with pytest.raises(ValueError):
        simple_matrix.generate(bad_mixture)


def test_reproducibility():
    """Test that results are reproducible with same random seed."""
    torch.manual_seed(42)
    matrix1 = TransitionMatrix.create_uniform(
        vocab_size=6, n_topics=2, color_fractions=[1, 1]
    )
    result1 = matrix1.generate(torch.ones(1, 2) / 2)

    torch.manual_seed(42)
    matrix2 = TransitionMatrix.create_uniform(
        vocab_size=6, n_topics=2, color_fractions=[1, 1]
    )
    result2 = matrix2.generate(torch.ones(1, 2) / 2)

    assert torch.allclose(result1, result2)



---
File: tests/test_vocab_builder.py
---
# faux_lingo/tests/test_vocab_builder.py
"""Tests for vocabulary building functionality."""

import pytest

from faux_lingo.core.vocab_builder import BuilderConfig, VocabBuilder, create_word_hierarchy


@pytest.fixture
def simple_config():
    """Create simple builder configuration."""
    return BuilderConfig(
        token_vocab_size=4,
        sequence_lengths=[2, 2],  # Two levels, sequences of length 2
        vocab_sizes=[4, 3],  # 4 chars from tokens, 3 words from chars
    )


def test_config_validation():
    """Test builder configuration validation."""
    # Valid configuration
    config = BuilderConfig(
        token_vocab_size=4,
        sequence_lengths=[2, 2],
        vocab_sizes=[4, 3],
    )
    assert config.token_vocab_size == 4

    # Invalid token vocab size
    with pytest.raises(ValueError, match="token_vocab_size must be positive"):
        BuilderConfig(
            token_vocab_size=0,
            sequence_lengths=[2],
            vocab_sizes=[4],
        )

    # Mismatched lengths
    with pytest.raises(ValueError, match="sequence length and vocabulary size"):
        BuilderConfig(
            token_vocab_size=4,
            sequence_lengths=[2],
            vocab_sizes=[4, 3],
        )

    # Invalid sequence length
    with pytest.raises(ValueError, match="sequence lengths must be positive"):
        BuilderConfig(
            token_vocab_size=4,
            sequence_lengths=[0, 2],
            vocab_sizes=[4, 3],
        )

    # Vocabulary too large for combinations
    with pytest.raises(ValueError, match="exceeds maximum possible combinations"):
        BuilderConfig(
            token_vocab_size=2,  # Only 4 possible pairs
            sequence_lengths=[2],
            vocab_sizes=[5],  # Want 5 unique pairs
        )


def test_builder_reproducibility(simple_config):
    """Test that building is reproducible with same seed."""
    config1 = BuilderConfig(
        **{**simple_config.__dict__, "seed": 42}
    )
    hierarchy1 = VocabBuilder(config1).build()

    config2 = BuilderConfig(
        **{**simple_config.__dict__, "seed": 42}
    )
    hierarchy2 = VocabBuilder(config2).build()

    # Check sequences match at each level
    for level1, level2 in zip(hierarchy1, hierarchy2):
        assert level1.sequences == level2.sequences


def test_sequence_uniqueness(simple_config):
    """Test that generated sequences are unique within levels."""
    hierarchy = VocabBuilder(simple_config).build()

    for level in hierarchy:
        sequences = set(level.sequences.values())
        assert len(sequences) == level.vocab_size


def test_sequence_validity(simple_config):
    """Test that sequences use valid tokens from previous level."""
    hierarchy = VocabBuilder(simple_config).build()

    # Check first level uses valid base tokens
    for sequence in hierarchy[0].sequences.values():
        assert all(0 <= token < simple_config.token_vocab_size for token in sequence)

    # Check second level uses valid tokens from first level
    for sequence in hierarchy[1].sequences.values():
        assert all(0 <= token < hierarchy[0].vocab_size for token in sequence)


def test_create_word_hierarchy():
    """Test convenience function for word hierarchy creation."""
    hierarchy = create_word_hierarchy(
        token_vocab_size=5,
        n_chars=10,
        n_words=20,
        chars_per_word=3,
        seed=42,
    )

    assert len(hierarchy) == 2  # Two levels: chars and words
    assert hierarchy[0].vocab_size == 10  # Number of characters
    assert hierarchy[1].vocab_size == 20  # Number of words
    assert all(len(seq) == 3 for seq in hierarchy[1].sequences.values())


def test_default_config():
    """Test default configuration creation."""
    config = VocabBuilder.create_default_config()
    
    # Verify defaults are valid
    hierarchy = VocabBuilder(config).build()
    assert len(hierarchy) == 3  # Three levels
    assert hierarchy[0].vocab_size == 20  # First level vocab size
    assert hierarchy[1].vocab_size == 15  # Second level vocab size
    assert hierarchy[2].vocab_size == 10  # Third level vocab size



---
File: tests/test_vocab_extensions.py
---
# faux_lingo/tests/test_vocab_extensions.py
"""Tests for vocabulary extension functionality."""

import pytest
import torch

from faux_lingo.core.vocab_builder import create_word_hierarchy
from faux_lingo.core.vocab_extensions import (
    AugmentationConfig,
    MultiMappingHierarchy,
    MultiMappingLevel,
    SequenceAugmenter,
    convert_to_multi_mapping,
)


@pytest.fixture
def simple_multi_level():
    """Create simple multi-mapping level for testing."""
    return MultiMappingLevel(
        vocab_size=2,
        chunk_size=2,
        sequences={
            0: [((0, 1), 0.7), ((1, 0), 0.3)],  # Two variants for token 0
            1: [((1, 1), 1.0)],  # Single mapping for token 1
        },
    )


@pytest.fixture
def simple_augmenter():
    """Create sequence augmenter with test configuration."""
    config = AugmentationConfig(
        deletion_prob=0.2,
        insertion_prob=0.2,
        substitution_prob=0.2,
        transposition_prob=0.2,
        seed=42,
    )
    return SequenceAugmenter(vocab_size=4, config=config)


def test_multi_level_validation():
    """Test validation of multi-mapping level properties."""
    # Valid level
    level = MultiMappingLevel(
        vocab_size=1,
        chunk_size=2,
        sequences={0: [((0, 1), 1.0)]},
    )
    assert level.vocab_size == 1

    # Invalid probabilities
    with pytest.raises(ValueError, match="do not sum to 1"):
        MultiMappingLevel(
            vocab_size=1,
            chunk_size=2,
            sequences={0: [((0, 1), 0.5)]},  # Prob < 1
        )

    # No sequences
    with pytest.raises(ValueError, match="No sequences defined"):
        MultiMappingLevel(
            vocab_size=1,
            chunk_size=2,
            sequences={0: []},
        )


def test_multi_hierarchy_decoding(simple_multi_level):
    """Test sequence decoding with multiple mappings."""
    hierarchy = MultiMappingHierarchy([simple_multi_level], device="cpu")
    
    # Create test sequence
    tokens = torch.tensor([[0, 1]], device="cpu")
    
    # Test reproducibility with seed
    torch.manual_seed(42)
    result1 = hierarchy.decode_sequence(tokens, start_level=0, target_level=0)
    
    torch.manual_seed(42)
    result2 = hierarchy.decode_sequence(tokens, start_level=0, target_level=0)
    
    assert torch.equal(result1, result2)


def test_augmenter_operations(simple_augmenter):
    """Test individual augmentation operations."""
    sequence = [0, 1, 2, 3]

    # Test deletion
    deleted = simple_augmenter._delete(sequence.copy())
    assert len(deleted) == len(sequence) - 1

    # Test insertion
    inserted = simple_augmenter._insert(sequence.copy())
    assert len(inserted) == len(sequence) + 1

    # Test substitution
    substituted = simple_augmenter._substitute(sequence.copy())
    assert len(substituted) == len(sequence)
    assert substituted != sequence

    # Test transposition
    transposed = simple_augmenter._transpose(sequence.copy())
    assert len(transposed) == len(sequence)
    assert transposed != sequence


def test_augmenter_sequence_handling(simple_augmenter):
    """Test sequence augmentation edge cases."""
    # Empty sequence
    assert simple_augmenter.augment_sequence(()) == ()

    # Single token
    single = (0,)
    augmented = simple_augmenter.augment_sequence(single)
    assert isinstance(augmented, tuple)
    assert len(augmented) > 0

    # Reproducibility
    torch.manual_seed(42)
    result1 = simple_augmenter.augment_sequence((0, 1, 2))
    
    torch.manual_seed(42)
    result2 = simple_augmenter.augment_sequence((0, 1, 2))
    
    assert result1 == result2


def test_hierarchy_conversion():
    """Test conversion from standard to multi-mapping hierarchy."""
    # Create simple word hierarchy
    hierarchy = create_word_hierarchy(
        token_vocab_size=4,
        n_chars=3,
        n_words=2,
        chars_per_word=2,
        seed=42,
    )

    # Create augmenter for variants
    config = AugmentationConfig(
        deletion_prob=0.1,
        insertion_prob=0.1,
        substitution_prob=0.1,
        transposition_prob=0.1,
        seed=42,
    )
    augmenter = SequenceAugmenter(vocab_size=4, config=config)

    # Convert to multi-mapping
    multi_hierarchy = convert_to_multi_mapping(
        hierarchy,
        augmenter=augmenter,
        n_variants=3,
    )

    # Check structure preserved
    assert len(multi_hierarchy.levels) == len(hierarchy.levels)
    
    # Check variants generated
    for level in multi_hierarchy.levels:
        for token, variants in level.sequences.items():
            # At least original sequence plus some variants
            assert len(variants) > 1
            # Probabilities sum to 1
            probs = sum(prob for _, prob in variants)
            assert torch.isclose(torch.tensor(probs), torch.tensor(1.0))


def test_augmentation_config_validation():
    """Test validation of augmentation configuration."""
    # Valid config
    config = AugmentationConfig(
        deletion_prob=0.1,
        insertion_prob=0.1,
        substitution_prob=0.1,
        transposition_prob=0.1,
    )
    assert config.deletion_prob == 0.1

    # Invalid probability value
    with pytest.raises(ValueError, match="between 0 and 1"):
        AugmentationConfig(deletion_prob=1.5)

    # Sum too large
    with pytest.raises(ValueError, match="must not exceed 1"):
        AugmentationConfig(
            deletion_prob=0.3,
            insertion_prob=0.3,
            substitution_prob=0.3,
            transposition_prob=0.3,
        )


def test_device_handling():
    """Test device placement and consistency."""
    # Create hierarchy on CPU
    level = MultiMappingLevel(
        vocab_size=1,
        chunk_size=2,
        sequences={0: [((0, 1), 1.0)]},
    )
    hierarchy = MultiMappingHierarchy([level], device="cpu")

    # Test decoding maintains device
    tokens = torch.tensor([[0]], device="cpu")
    result = hierarchy.decode_sequence(tokens, start_level=0, target_level=0)
    assert result.device.type == "cpu"

    # Test with same level returns input unchanged
    result = hierarchy.decode_sequence(tokens, start_level=0, target_level=0)
    assert result.device.type == "cpu"



---
File: tests/test_vocab_mapping.py
---
# faux_lingo/tests/test_vocab_mapping.py
"""Tests for vocabulary mapping and decoding."""

import pytest
import torch

from faux_lingo.core.vocab_mapping import VocabHierarchy, VocabLevel

@pytest.fixture
def simple_hierarchy():
    """Create a simple 3-level hierarchy for testing.
    
    Structure:
    - Level 0 (most abstract) tokens map to 2 level 1 tokens
    - Level 1 tokens map to 2 level 2 (most concrete) tokens
    
    Example mappings:
    Level 0 -> Level 1:
    - 0 -> (0, 1)
    - 1 -> (1, 2)
    
    Level 1 -> Level 2:
    - 0 -> (0, 1)
    - 1 -> (2, 3)
    - 2 -> (3, 4)
    """
    levels = [
        VocabLevel(  # Level 0 -> Level 1 mapping
            vocab_size=2,
            chunk_size=2,
            sequences={
                0: (0, 1),  # word 0 -> chars [0,1]
                1: (1, 2),  # word 1 -> chars [1,2]
            },
        ),
        VocabLevel(  # Level 1 -> Level 2 mapping
            vocab_size=3,
            chunk_size=2,
            sequences={
                0: (0, 1),  # char 0 -> tokens [0,1]
                1: (2, 3),  # char 1 -> tokens [2,3]
                2: (3, 4),  # char 2 -> tokens [3,4]
            },
        ),
    ]
    return VocabHierarchy(levels)

def test_vocab_level_validation():
    """Test validation of vocabulary level properties."""
    # Valid level
    level = VocabLevel(vocab_size=2, chunk_size=1, sequences={0: (0,), 1: (1,)})
    assert level.max_sequence_length == 1

    # Invalid vocab size
    with pytest.raises(ValueError, match="vocab_size must be positive"):
        VocabLevel(vocab_size=0, chunk_size=1, sequences={})

    # Invalid chunk size
    with pytest.raises(ValueError, match="chunk_size must be positive"):
        VocabLevel(vocab_size=1, chunk_size=0, sequences={0: (0,)})

    # Mismatched sequence count
    with pytest.raises(ValueError, match="Number of sequences"):
        VocabLevel(vocab_size=2, chunk_size=1, sequences={0: (0,)})

    # Invalid sequence type
    with pytest.raises(ValueError, match="must be a tuple"):
        VocabLevel(vocab_size=1, chunk_size=1, sequences={0: [0]})

    # Invalid sequence elements
    with pytest.raises(ValueError, match="must be integers"):
        VocabLevel(vocab_size=1, chunk_size=1, sequences={0: (0.5,)})

def test_hierarchy_respected():
    import torch
    import numpy as np
    from faux_lingo.core.vocab_builder import BuilderConfig, VocabBuilder
    
    config = BuilderConfig(
        token_vocab_size=10,
        sequence_lengths=[2, 3],  # Length at each level
        vocab_sizes=[20, 30]      # Size of each level
    )
    
    builder = VocabBuilder(config)
    hierarchy = builder.build()
    
    tokens = torch.tensor([[0,1,2]])
    decoded = hierarchy.decode_sequence(tokens,start_level=0, target_level=2)
    
    target_length = np.prod( [level.chunk_size for level in hierarchy.levels]) * tokens.shape[1]
    assert target_length == decoded.shape[1]
    
def test_single_token_decoding(simple_hierarchy):
    """Test decoding of individual tokens."""
    # Level 0 token 0 maps to level 1 tokens [0,1]
    # which map to level 2 tokens [0,1,2,3]
    word = torch.tensor([[0]], device=simple_hierarchy.device)
    
    # Default decoding (full expansion)
    tokens = simple_hierarchy.decode_sequence(word)
    assert torch.equal(tokens, torch.tensor([[0, 1, 2, 3]], device=simple_hierarchy.device))
    
    # Decode from level 0 to level 1
    chars = simple_hierarchy.decode_sequence(word, target_level=1)
    assert torch.equal(chars, torch.tensor([[0, 1]], device=simple_hierarchy.device))


def test_sequence_decoding(simple_hierarchy):
    """Test decoding of token sequences."""
    # Level 0 sequence [0,1] maps to:
    # Level 1: [0,1,1,2]
    # Level 2: [0,1,2,3,2,3,3,4]
    words = torch.tensor([[0, 1]], device=simple_hierarchy.device)
    
    # Default decoding (full expansion)
    tokens = simple_hierarchy.decode_sequence(words)
    assert torch.equal(tokens, 
        torch.tensor([[0, 1, 2, 3, 2, 3, 3, 4]], device=simple_hierarchy.device))
    
    # Decode to intermediate level
    chars = simple_hierarchy.decode_sequence(words, target_level=1)
    assert torch.equal(chars, torch.tensor([[0, 1, 1, 2]], device=simple_hierarchy.device))


def test_batch_decoding(simple_hierarchy):
    """Test decoding of batched sequences."""
    words = torch.tensor([
        [0, 1],  # First sequence: word 0 followed by word 1
        [1, 0],  # Second sequence: word 1 followed by word 0
    ], device=simple_hierarchy.device)
    
    # Default decoding (full expansion)
    tokens = simple_hierarchy.decode_sequence(words)
    
    expected = torch.tensor([
        [0, 1, 2, 3, 2, 3, 3, 4],  # Decoded first sequence
        [2, 3, 3, 4, 0, 1, 2, 3],  # Decoded second sequence
    ], device=simple_hierarchy.device)
    
    assert torch.equal(tokens, expected)


def test_invalid_level_decoding(simple_hierarchy):
    """Test validation of decoding levels."""
    words = torch.tensor([[0]], device=simple_hierarchy.device)
    
    # Invalid start level (too high)
    with pytest.raises(ValueError, match="Invalid start_level"):
        simple_hierarchy.decode_sequence(words, start_level=3)
        
    # Invalid target level (negative)
    with pytest.raises(ValueError, match="Invalid target_level"):
        simple_hierarchy.decode_sequence(words, target_level=-1)
    
    # Can't decode upward
    with pytest.raises(ValueError, match="Can only decode to same or higher levels"):
        simple_hierarchy.decode_sequence(words, start_level=1, target_level=0)


def test_default_decoding(simple_hierarchy):
    """Test default decoding behavior."""
    # Single token at most abstract level
    word = torch.tensor([[0]], device=simple_hierarchy.device)
    
    # These should all be equivalent
    full_decode = simple_hierarchy.decode_sequence(word)
    explicit_decode = simple_hierarchy.decode_sequence(word, start_level=0, target_level=2)
    
    assert torch.equal(full_decode, explicit_decode)
    assert torch.equal(full_decode, torch.tensor([[0, 1, 2, 3]], device=simple_hierarchy.device))


def test_from_sequences():
    """Test creation of hierarchy from sequence mappings."""
    sequences = [
        {0: (0,), 1: (1,)},  # Base tokens
        {0: (0, 1)},  # One character
    ]
    chunk_sizes = [1, 2]

    hierarchy = VocabHierarchy.from_sequences(sequences, chunk_sizes)
    assert len(hierarchy) == 2
    assert hierarchy[0].vocab_size == 2
    assert hierarchy[1].vocab_size == 1

    # Mismatched lengths
    with pytest.raises(ValueError, match="chunk size for each level"):
        VocabHierarchy.from_sequences(sequences, chunk_sizes[:-1])


def test_device_handling():
    """Test device placement and movement of tensors."""
    # Create two-level hierarchy for testing
    levels = [
        VocabLevel(  # Base tokens
            vocab_size=2,
            chunk_size=1,
            sequences={0: (0,), 1: (1,)},
        ),
        VocabLevel(  # Higher level tokens
            vocab_size=2,
            chunk_size=2,
            sequences={0: (0, 1), 1: (1, 0)},
        ),
    ]
    hierarchy = VocabHierarchy(levels, device="cpu")

    # Test decoding maintains device
    tokens = torch.tensor([[0]], device="cpu")
    result = hierarchy.decode_sequence(tokens, start_level=0, target_level=1)
    assert result.device.type == "cpu"

    # Test with same level returns input unchanged
    # ...this should probably be its own test, no? has nothing to do with device mgmt.
    tokens = torch.tensor([[0]], device="cpu")
    result = hierarchy.decode_sequence(tokens, start_level=0, target_level=0)
    assert result.device.type == "cpu"

    # Input on different device gets moved
    if torch.cuda.is_available():
        hierarchy = VocabHierarchy(levels, device="cuda")
        result = hierarchy.decode_sequence(tokens, start_level=1, target_level=0)
        assert result.device.type == "cuda"


